{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "(WithMask)(김웅곤)(TPU-Effnet-B7-512-32)Covid19-StudyBaseLine.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOWeON8dE++HAtTl3OMlwNX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimwoonggon/SIIM-COVID19-KAGGLE/blob/main/(WithMask)(%EA%B9%80%EC%9B%85%EA%B3%A4)(TPU_Effnet_B7_512_32)Covid19_StudyBaseLine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhbJWFGLTJ9A",
        "outputId": "1ba29b79-ef41-476a-961d-0a307a7e35bd"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rU_vL8ic3HcZ",
        "outputId": "a7791da1-8a15-4dd1-cdf0-81a44bb4ca7d"
      },
      "source": [
        "#!pip install tensorflow~=2.2.0 tensorflow_gcs_config~=2.2.0\n",
        "#!pip install -U tensorflow-addons==0.9.1\n",
        "!pip install -U tensorflow-addons\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow_addons as tfa\n",
        "import requests\n",
        "import os\n",
        "resp = requests.post(\"http://{}:8475/requestversion/{}\".format(os.environ[\"COLAB_TPU_ADDR\"].split(\":\")[0], tf.__version__))\n",
        "if resp.status_code != 200:\n",
        "  print(\"Failed to switch the TPU to TF {}\".format(version))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-addons\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/4b/e893d194e626c24b3df2253066aa418f46a432fdb68250cde14bf9bb0700/tensorflow_addons-0.13.0-cp37-cp37m-manylinux2010_x86_64.whl (679kB)\n",
            "\u001b[K     |████████████████████████████████| 686kB 5.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6xX204k2Ktd"
      },
      "source": [
        "!pip install -q efficientnet >> /dev/null"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFPVid4aN0du",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a1799bd-bb06-46c4-afeb-dfba9c05d5b2"
      },
      "source": [
        "import random, re, math\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf, tensorflow.keras.backend as K\n",
        "!pip install gcsfs #gcp 파일 로드\n",
        "#from kaggle_datasets import KaggleDatasets\n",
        "from tensorflow.data.experimental import AUTOTUNE\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "import operator\n",
        "import gc\n",
        "import pathlib\n",
        "from scipy import spatial\n",
        "import cv2\n",
        "import functools"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gcsfs\n",
            "  Downloading https://files.pythonhosted.org/packages/11/30/306bcaadd0145f55934202c77215e26e73b4c3d81fbdac587d26af38a2ad/gcsfs-2021.6.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.7/dist-packages (from gcsfs) (0.4.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gcsfs) (2.23.0)\n",
            "Collecting fsspec==2021.06.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/3a/666e63625a19883ae8e1674099e631f9737bd5478c4790e5ad49c5ac5261/fsspec-2021.6.1-py3-none-any.whl (115kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from gcsfs) (4.4.2)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.7/dist-packages (from gcsfs) (1.31.0)\n",
            "Collecting aiohttp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/c0/5890b4c8b04a79b7360e8fe4490feb0bb3ab179743f199f0e6220cebd568/aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 10.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib->gcsfs) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs) (3.0.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (0.2.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (4.2.2)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (57.0.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs) (21.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs) (3.7.4.3)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a6/4123b8165acbe773d1a8dc8e3f0d1edea16d29f7de018eda769abb56bd30/multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 26.9MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/62/046834c5fc998c88ab2ef722f5d42122230a632212c8afa76418324f53ff/yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 26.4MB/s \n",
            "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.1.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth>=1.2->gcsfs) (0.4.8)\n",
            "Installing collected packages: fsspec, multidict, yarl, async-timeout, aiohttp, gcsfs\n",
            "Successfully installed aiohttp-3.7.4.post0 async-timeout-3.0.1 fsspec-2021.6.1 gcsfs-2021.6.1 multidict-5.1.0 yarl-1.6.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NERnp6GCrxgr"
      },
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    tf.random.set_seed(seed)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37CFiVVS4Q82",
        "outputId": "5921a060-d31a-4036-c7c2-def7edffd620"
      },
      "source": [
        "DEVICE = \"TPU\"\n",
        "if DEVICE == \"TPU\":\n",
        "    print(\"connecting to TPU...\")\n",
        "    try:\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "        print('Running on TPU ', tpu.master())\n",
        "    except ValueError:\n",
        "        print(\"Could not connect to TPU\")\n",
        "        tpu = None\n",
        "\n",
        "    if tpu:\n",
        "        try:\n",
        "            print(\"initializing  TPU ...\")\n",
        "            tf.config.experimental_connect_to_cluster(tpu)\n",
        "            tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "            print(\"TPU initialized\")\n",
        "        except:\n",
        "            print(\"failed to initialize TPU\")\n",
        "    else:\n",
        "        DEVICE = \"GPU\"\n",
        "\n",
        "if DEVICE != \"TPU\":\n",
        "    print(\"Using default strategy for CPU and single GPU\")\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "if DEVICE == \"GPU\":\n",
        "    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "    \n",
        "\n",
        "AUTO     = tf.data.experimental.AUTOTUNE\n",
        "REPLICAS = strategy.num_replicas_in_sync\n",
        "print(f'REPLICAS: {REPLICAS}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "connecting to TPU...\n",
            "Running on TPU  grpc://10.12.153.26:8470\n",
            "initializing  TPU ...\n",
            "INFO:tensorflow:Initializing the TPU system: grpc://10.12.153.26:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.12.153.26:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TPU initialized\n",
            "REPLICAS: 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8Ylvljm1eIu",
        "outputId": "a3800a5f-18f1-42b6-d986-e3d922d076ca"
      },
      "source": [
        "MIXED_PRECISION = True\n",
        "XLA_ACCELERATE = True\n",
        " \n",
        "if MIXED_PRECISION:\n",
        "    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
        "    if tpu: \n",
        "        policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\n",
        "    else: \n",
        "        policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
        "    mixed_precision.set_policy(policy)\n",
        "    print('Mixed precision enabled')\n",
        " \n",
        "if XLA_ACCELERATE:\n",
        "    tf.config.optimizer.set_jit(True)\n",
        "    print('Accelerated Linear Algebra enabled')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mixed precision enabled\n",
            "Accelerated Linear Algebra enabled\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quqZy4Kipt7v"
      },
      "source": [
        "class CFG:\n",
        "    StudyOrTwoClasses = \"study\"\n",
        "    WIDTH = 640\n",
        "    HEIGHT = 640\n",
        "    OBJ_WIDTH = 512\n",
        "    OBJ_HEIGHT = 512\n",
        "    MASK_WIDTH = 32\n",
        "    MASK_HEIGHT = 32\n",
        "    MASK_OBJ_WIDTH = 32\n",
        "    MASK_OBJ_HEIGHT = 32\n",
        "    MEAN = (0.485, 0.456, 0.406)\n",
        "    STD = (0.229, 0.224, 0.225)\n",
        "    CHANNELS = 3\n",
        "    \n",
        "    REPLICAS = 8\n",
        "    EPOCHS = 20\n",
        "    BATCH_SIZE = 8 * REPLICAS\n",
        "    AUG_BATCH = BATCH_SIZE\n",
        "    \n",
        "    LEARNING_RATE = 9e-5 * REPLICAS\n",
        "    \n",
        "    NUMBER_OF_CLASSES = 4\n",
        "    RANDAUG_NUM = 2\n",
        "    RANDAUG_MAGNITUDE = 15\n",
        "    N_FOLDS = 5\n",
        " \n",
        "    NET = 7\n",
        "    TTA_NUM = 4\n",
        "    SEED = 100\n",
        "    GCS_PATH = 'gs://kds-d01a8ccac2f40923bd1e45b9926e5dad01232e523339871333f1162e'\n",
        "    ROOT_PATH = 'gdrive/My Drive/Colab Notebooks/KAGGLE_COVID19'\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qXd8pFJ1fmR",
        "outputId": "eff9cee3-9aaf-4a26-d8be-885dd3d92a32"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import efficientnet.tfkeras as efn\n",
        "import tensorflow.keras.applications as apl\n",
        "# import EfficientNetB0\n",
        "# Configuration\n",
        "effnets = [efn.EfficientNetB0,efn.EfficientNetB1,efn.EfficientNetB2,efn.EfficientNetB3,efn.EfficientNetB4,efn.EfficientNetB5,efn.EfficientNetB6,efn.EfficientNetB7]\n",
        "#effnets = [apl.EfficientNetB0,apl.EfficientNetB1,apl.EfficientNetB2,apl.EfficientNetB3,apl.EfficientNetB4,apl.EfficientNetB5,apl.EfficientNetB6,apl.EfficientNetB7]\n",
        "TTA_NUM = CFG.TTA_NUM\n",
        "TOTALWIDTH = CFG.WIDTH\n",
        "TOTALHEIGHT = CFG.HEIGHT\n",
        "HEIGHT = CFG.OBJ_HEIGHT\n",
        "WIDTH = CFG.OBJ_WIDTH\n",
        "IMAGE_SIZE = [HEIGHT, WIDTH]\n",
        "NET = CFG.NET\n",
        "BATCH_SIZE = CFG.BATCH_SIZE\n",
        "AUG_BATCH = BATCH_SIZE\n",
        "CHANNELS = CFG.CHANNELS\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        " \n",
        "GCS_PATH = CFG.GCS_PATH\n",
        "ROOT_PATH = CFG.ROOT_PATH\n",
        "EPOCHS = CFG.EPOCHS\n",
        "SEED = CFG.SEED\n",
        "LEARNING_RATE = CFG.LEARNING_RATE\n",
        "NUMBER_OF_CLASSES = CFG.NUMBER_OF_CLASSES\n",
        " \n",
        "#class_weight = CFG.CLASS_WEIGHT\n",
        " \n",
        "IMAGE_MEAN = CFG.MEAN\n",
        "IMAGE_STD = CFG.STD \n",
        "FILENAMES = tf.io.gfile.glob(CFG.GCS_PATH+f\"/train*\")\n",
        "#TEST_FILENAMES = tf.io.gfile.glob(CFG.GCS_PATH+\"/test*\")\n",
        "print(FILENAMES)\n",
        "#print(TEST_FILENAMES)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['gs://kds-d01a8ccac2f40923bd1e45b9926e5dad01232e523339871333f1162e/train_mask00-1267.tfrec', 'gs://kds-d01a8ccac2f40923bd1e45b9926e5dad01232e523339871333f1162e/train_mask01-1267.tfrec', 'gs://kds-d01a8ccac2f40923bd1e45b9926e5dad01232e523339871333f1162e/train_mask02-1267.tfrec', 'gs://kds-d01a8ccac2f40923bd1e45b9926e5dad01232e523339871333f1162e/train_mask03-1267.tfrec', 'gs://kds-d01a8ccac2f40923bd1e45b9926e5dad01232e523339871333f1162e/train_mask04-1266.tfrec']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SOJUTcaXH2a"
      },
      "source": [
        "test_image = tf.cast(tf.random.uniform(shape=(1024,1024,3),minval = 0,maxval = 255,dtype=tf.int32), dtype=tf.uint8)\n",
        " \n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from tensorflow_addons.image.utils import to_4D_image, from_4D_image \n",
        "import inspect\n",
        "import math\n",
        "#import tensorflow.compat.v1 as tf\n",
        "#from tensorflow.contrib import image as contrib_image\n",
        "#from tensorflow.contrib import training as contrib_training\n",
        "def blend(image1, image2, factor):\n",
        "  \"\"\"Blend image1 and image2 using 'factor'.\n",
        "  Factor can be above 0.0.  A value of 0.0 means only image1 is used.\n",
        "  A value of 1.0 means only image2 is used.  A value between 0.0 and\n",
        "  1.0 means we linearly interpolate the pixel values between the two\n",
        "  images.  A value greater than 1.0 \"extrapolates\" the difference\n",
        "  between the two pixel values, and we clip the results to values\n",
        "  between 0 and 255.\n",
        "  Args:\n",
        "    image1: An image Tensor of type uint8.\n",
        "    image2: An image Tensor of type uint8.\n",
        "    factor: A floating point value above 0.0.\n",
        "  Returns:\n",
        "    A blended image Tensor of type uint8.\n",
        "  \"\"\"\n",
        "  if factor == 0.0:\n",
        "    return tf.convert_to_tensor(image1)\n",
        "  if factor == 1.0:\n",
        "    return tf.convert_to_tensor(image2)\n",
        " \n",
        "  image1 = tf.cast(image1, dtype=tf.float32)\n",
        "  image2 = tf.cast(image2, dtype=tf.float32)\n",
        " \n",
        "  difference = image2 - image1\n",
        "  scaled = factor * difference\n",
        " \n",
        "  # Do addition in float.\n",
        "  temp = tf.cast(image1, dtype=tf.float32) + scaled\n",
        " \n",
        "  # Interpolate\n",
        "  if factor > 0.0 and factor < 1.0:\n",
        "    # Interpolation means we always stay within 0 and 255.\n",
        "    return tf.cast(temp, tf.uint8)\n",
        " \n",
        "  # Extrapolate:\n",
        "  #\n",
        "  # We need to clip and then cast.\n",
        "  return tf.cast(tf.clip_by_value(temp, 0.0, 255.0), tf.uint8)\n",
        "def Identity(image, _):\n",
        "    return image\n",
        "#Identity(test_image, 3)\n",
        "def AutoContrast(image, _):\n",
        "  \"\"\"Implements Autocontrast function from PIL using TF ops.\n",
        "  Args:\n",
        "    image: A 3D uint8 tensor.\n",
        "  Returns:\n",
        "    The image after it has had autocontrast applied to it and will be of type\n",
        "    uint8.\n",
        "  \"\"\"\n",
        " \n",
        "  def scale_channel(image):\n",
        "    \"\"\"Scale the 2D image using the autocontrast rule.\"\"\"\n",
        "    # A possibly cheaper version can be done using cumsum/unique_with_counts\n",
        "    # over the histogram values, rather than iterating over the entire image.\n",
        "    # to compute mins and maxes.\n",
        "    lo = tf.cast(tf.reduce_min(image), dtype = tf.float32)\n",
        "    hi = tf.cast(tf.reduce_max(image), dtype = tf.float32)\n",
        " \n",
        "    # Scale the image, making the lowest value 0 and the highest value 255.\n",
        "    def scale_values(im):\n",
        "        scale = 255.0 / (hi - lo)\n",
        "        offset = -lo * scale\n",
        "        im = tf.cast(im, dtype=tf.float32) * scale + offset\n",
        "        im = tf.clip_by_value(im, 0.0, 255.0)\n",
        "        return tf.cast(im, tf.uint8)\n",
        " \n",
        "    result = tf.cond(hi > lo, lambda: scale_values(image), lambda: image)\n",
        "    return result\n",
        " \n",
        "  # Assumes RGB for now.  Scales each channel independently\n",
        "  # and then stacks the result.\n",
        "  s1 = scale_channel(image[:, :, 0])\n",
        "  s2 = scale_channel(image[:, :, 1])\n",
        "  s3 = scale_channel(image[:, :, 2])\n",
        "  image = tf.stack([s1, s2, s3], 2)\n",
        "  return image\n",
        " \n",
        "AutoContrast(test_image, 3)\n",
        "def Equalize(image, _):\n",
        "  \"\"\"Implements Equalize function from PIL using TF ops.\"\"\"\n",
        "  def scale_channel(im, c):\n",
        "    \"\"\"Scale the data in the channel to implement equalize.\"\"\"\n",
        "    im = tf.cast(im[:, :, c], tf.int32)\n",
        "    # Compute the histogram of the image channel.\n",
        "    histo = tf.histogram_fixed_width(im, [0, 255], nbins=256)\n",
        " \n",
        "    # For the purposes of computing the step, filter out the nonzeros.\n",
        "    nonzero = tf.where(tf.not_equal(histo, 0))\n",
        "    nonzero_histo = tf.reshape(tf.gather(histo, nonzero), [-1])\n",
        "    step = (tf.reduce_sum(nonzero_histo) - nonzero_histo[-1]) // 255\n",
        " \n",
        "    def build_lut(histo, step):\n",
        "      # Compute the cumulative sum, shifting by step // 2\n",
        "      # and then normalization by step.\n",
        "      lut = (tf.cumsum(histo) + (step // 2)) // step\n",
        "      # Shift lut, prepending with 0.\n",
        "      lut = tf.concat([[0], lut[:-1]], 0)\n",
        "      # Clip the counts to be in range.  This is done\n",
        "      # in the C code for image.point.\n",
        "      return tf.clip_by_value(lut, 0, 255)\n",
        " \n",
        "    # If step is zero, return the original image.  Otherwise, build\n",
        "    # lut from the full histogram and step and then index from it.\n",
        "    result = tf.cond(tf.equal(step, 0),\n",
        "                     lambda: im,\n",
        "                     lambda: tf.gather(build_lut(histo, step), im))\n",
        " \n",
        "    return tf.cast(result, tf.uint8)\n",
        " \n",
        "  # Assumes RGB for now.  Scales each channel independently\n",
        "  # and then stacks the result.\n",
        "  s1 = scale_channel(image, 0)\n",
        "  s2 = scale_channel(image, 1)\n",
        "  s3 = scale_channel(image, 2)\n",
        "  image = tf.stack([s1, s2, s3], 2)\n",
        "  return image\n",
        "Equalize(test_image, 1)\n",
        "def Rotate(image, degrees):\n",
        "  \"\"\"Rotates the image by degrees either clockwise or counterclockwise.\n",
        "  Args:\n",
        "    image: An image Tensor of type uint8.\n",
        "    degrees: Float, a scalar angle in degrees to rotate all images by. If\n",
        "      degrees is positive the image will be rotated clockwise otherwise it will\n",
        "      be rotated counterclockwise.\n",
        "    replace: A one or three value 1D tensor to fill empty pixels caused by\n",
        "      the rotate operation.\n",
        "  Returns:\n",
        "    The rotated version of image.\n",
        "  \"\"\"\n",
        "  # Convert from degrees to radians.\n",
        "  degrees = int(degrees)\n",
        "  degrees_to_radians = math.pi / 180.0\n",
        "  radians = degrees * degrees_to_radians\n",
        " \n",
        "  # In practice, we should randomize the rotation degrees by flipping\n",
        "  # it negatively half the time, but that's done on 'degrees' outside\n",
        "  # of the function.\n",
        "  #image = contrib_image.rotate(wrap(image), radians)\n",
        "  image = tfa.image.rotate(image, radians)\n",
        "  #return unwrap(image, replace)\n",
        "  return image\n",
        "Rotate(test_image, 30.1)\n",
        "def Solarize(image, threshold=128):\n",
        "  # For each pixel in the image, select the pixel\n",
        "  # if the value is less than the threshold.\n",
        "  # Otherwise, subtract 255 from the pixel.\n",
        "  #image = tf.convert_to_tensor(image, dtype=tf.int32)\n",
        "  #print(image)\n",
        "  \n",
        "  threshold = tf.cast(threshold, dtype=tf.uint8)\n",
        "  #print(threshold)\n",
        "  minus_value = tf.constant(255, dtype=tf.uint8)\n",
        "  #print(minus_value)\n",
        "  return tf.where(image < threshold, image, minus_value - image)\n",
        "Solarize(test_image, 10.0)\n",
        "def Color(image, factor):\n",
        "  \"\"\"Equivalent of PIL Color.\"\"\"\n",
        "  degenerate = tf.image.grayscale_to_rgb(tf.image.rgb_to_grayscale(image))\n",
        "  #factor = tf.cast(factor, dtype=tf.float32)\n",
        "  return blend(degenerate, image, factor)\n",
        "Color(test_image, 10.1)\n",
        " \n",
        " \n",
        " \n",
        "def Posterize(image, bits):\n",
        " \n",
        "  bits=int(bits)\n",
        "  #print(bits)\n",
        "  \"\"\"Equivalent of PIL Posterize.\"\"\"\n",
        "  shift = 8 - bits\n",
        "  #print(shift)\n",
        "  #print(image)\n",
        "  return tf.bitwise.left_shift(tf.bitwise.right_shift(image, shift), shift)\n",
        "Posterize(test_image, 1.1)\n",
        "def Contrast(image, factor):\n",
        "  \"\"\"Equivalent of PIL Contrast.\"\"\"\n",
        "  degenerate = tf.image.rgb_to_grayscale(image)\n",
        "  # Cast before calling tf.histogram.\n",
        "  degenerate = tf.cast(degenerate, tf.int32)\n",
        " \n",
        "  # Compute the grayscale histogram, then compute the mean pixel value,\n",
        "  # and create a constant image size of that value.  Use that as the\n",
        "  # blending degenerate target of the original image.\n",
        "  hist = tf.histogram_fixed_width(degenerate, [0, 255], nbins=256)\n",
        "  mean = tf.reduce_sum(tf.cast(hist, tf.float32)) / 256.0\n",
        "  degenerate = tf.ones_like(degenerate, dtype=tf.float32) * mean\n",
        "  degenerate = tf.clip_by_value(degenerate, 0.0, 255.0)\n",
        "  degenerate = tf.image.grayscale_to_rgb(tf.cast(degenerate, tf.uint8))\n",
        "  return blend(degenerate, image, factor)\n",
        "Contrast(test_image, 10.1)\n",
        "def Brightness(image, factor):\n",
        "  \"\"\"Equivalent of PIL Brightness.\"\"\"\n",
        "  degenerate = tf.zeros_like(image)\n",
        "  return blend(degenerate, image, factor)\n",
        "Brightness(test_image, 10.1)\n",
        "def _sharpness_image(image, factor):\n",
        "    orig_image = image\n",
        "    image_dtype = image.dtype\n",
        "    image_channels = image.shape[-1]\n",
        "    image = tf.cast(image, tf.float32)\n",
        " \n",
        "    # SMOOTH PIL Kernel.\n",
        "    kernel = (\n",
        "        tf.constant(\n",
        "            [[1, 1, 1], [1, 5, 1], [1, 1, 1]], dtype=tf.float32, shape=[3, 3, 1, 1]\n",
        "        )\n",
        "        / 13.0\n",
        "    )\n",
        "    kernel = tf.tile(kernel, [1, 1, image_channels, 1])\n",
        " \n",
        "    # Apply kernel channel-wise.\n",
        "    degenerate = tf.nn.depthwise_conv2d(\n",
        "        image, kernel, strides=[1, 1, 1, 1], padding=\"VALID\", dilations=[1, 1]\n",
        "    )\n",
        "    degenerate = tf.cast(degenerate, image_dtype)\n",
        " \n",
        "    # For the borders of the resulting image, fill in the values of the original image.\n",
        "    mask = tf.ones_like(degenerate)\n",
        "    padded_mask = tf.pad(mask, [[0, 0], [1, 1], [1, 1], [0, 0]])\n",
        "    padded_degenerate = tf.pad(degenerate, [[0, 0], [1, 1], [1, 1], [0, 0]])\n",
        "    result = tf.where(tf.equal(padded_mask, 1), padded_degenerate, orig_image)\n",
        " \n",
        "    # Blend the final result.\n",
        "    blended = blend(result, orig_image, factor)\n",
        "    return tf.cast(blended, image_dtype)\n",
        " \n",
        " \n",
        "def Sharpness(image, factor):\n",
        "    \n",
        "        image_dims = tf.rank(image)\n",
        "        image = to_4D_image(image)\n",
        "        image = _sharpness_image(image, factor=factor)\n",
        "        return from_4D_image(image, image_dims)\n",
        "    #return tfa.image.sharpness(image, factor)\n",
        "Sharpness(test_image, 10.1)\n",
        " \n",
        "def ShearX(image, level):\n",
        "  \"\"\"Equivalent of PIL Shearing in X dimension.\"\"\"\n",
        "  # Shear parallel to x axis is a projective transform\n",
        "  # with a matrix form of:\n",
        "  # [1  level\n",
        "  #  0  1].\n",
        "  #image = contrib_image.transform(\n",
        "  #    wrap(image), [1., level, 0., 0., 1., 0., 0., 0.])\n",
        "  #return unwrap(image, replace)\n",
        "  \n",
        "  return tfa.image.shear_x(image, level, 0)\n",
        "ShearX(test_image,10)\n",
        "def ShearY(image, level):\n",
        "  \"\"\"Equivalent of PIL Shearing in Y dimension.\"\"\"\n",
        "  # Shear parallel to y axis is a projective transform\n",
        "  # with a matrix form of:\n",
        "  # [1  0\n",
        "  #  level  1].\n",
        "  #image = contrib_image.transform(\n",
        "  #    wrap(image), [1., 0., 0., level, 1., 0., 0., 0.])\n",
        "  #return unwrap(image, replace)\n",
        "  return tfa.image.shear_y(image, level, 0)\n",
        "ShearX(test_image,20)\n",
        "def wrap(image):\n",
        "  \"\"\"Returns 'image' with an extra channel set to all 1s.\"\"\"\n",
        "  shape = tf.shape(image)\n",
        "  extended_channel = tf.ones([shape[0], shape[1], 1], image.dtype)\n",
        "  extended = tf.concat([image, extended_channel], 2)\n",
        "  return extended\n",
        " \n",
        "def unwrap(image, replace):\n",
        "  \"\"\"Unwraps an image produced by wrap.\n",
        "  Where there is a 0 in the last channel for every spatial position,\n",
        "  the rest of the three channels in that spatial dimension are grayed\n",
        "  (set to 128).  Operations like translate and shear on a wrapped\n",
        "  Tensor will leave 0s in empty locations.  Some transformations look\n",
        "  at the intensity of values to do preprocessing, and we want these\n",
        "  empty pixels to assume the 'average' value, rather than pure black.\n",
        "  Args:\n",
        "    image: A 3D Image Tensor with 4 channels.\n",
        "    replace: A one or three value 1D tensor to fill empty pixels.\n",
        "  Returns:\n",
        "    image: A 3D image Tensor with 3 channels.\n",
        "  \"\"\"\n",
        "  image_shape = tf.shape(image)\n",
        "  # Flatten the spatial dimensions.\n",
        "  flattened_image = tf.reshape(image, [-1, image_shape[2]])\n",
        " \n",
        "  # Find all pixels where the last channel is zero.\n",
        "  alpha_channel = flattened_image[:, 3]\n",
        " \n",
        "  replace = tf.concat([replace, tf.ones([1], image.dtype)], 0)\n",
        " \n",
        "  # Where they are zero, fill them in with 'replace'.\n",
        "  flattened_image = tf.where(\n",
        "      tf.equal(alpha_channel, 0),\n",
        "      tf.ones_like(flattened_image, dtype=image.dtype) * replace,\n",
        "      flattened_image)\n",
        " \n",
        "  image = tf.reshape(flattened_image, image_shape)\n",
        "  image = tf.slice(image, [0, 0, 0], [image_shape[0], image_shape[1], 3])\n",
        "  return image\n",
        " \n",
        "def TranslateX(image, pixels):\n",
        "  \"\"\"Equivalent of PIL Translate in X dimension.\"\"\"\n",
        "  #image = contrib_image.translate(wrap(image), [-pixels, 0])\n",
        "  #return unwrap(image, replace)\n",
        "  return tfa.image.translate_xy(image, [pixels, 0], replace=0)\n",
        "TranslateX(test_image, 10)\n",
        "def TranslateY(image, pixels):\n",
        "  \"\"\"Equivalent of PIL Translate in Y dimension.\"\"\"\n",
        "  #image = contrib_image.translate(wrap(image), [0, -pixels])\n",
        "  #return unwrap(image, replace)\n",
        "  return tfa.image.translate_xy(image, [0, pixels], replace=0)\n",
        "#TranslateY(test_image, 10)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ws5Tg2JZpE7g"
      },
      "source": [
        "class RandomResizedCrop:\n",
        "    \"\"\"Torchvision's variant of crop a random part of the input and rescale it to some size.\n",
        "    Args:\n",
        "        height (int): height after crop and resize.\n",
        "        width (int): width after crop and resize.\n",
        "        scale ((float, float)): range of size of the origin size cropped\n",
        "        ratio ((float, float)): range of aspect ratio of the origin aspect ratio cropped\n",
        "        interpolation (OpenCV flag): flag that is used to specify the interpolation algorithm. Should be one of:\n",
        "            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n",
        "            Default: cv2.INTER_LINEAR.\n",
        "        p (float): probability of applying the transform. Default: 1.\n",
        "    Targets:\n",
        "        image, mask, bboxes, keypoints\n",
        "    Image types:\n",
        "        uint8, float32\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        height,\n",
        "        width,\n",
        "        org_height,\n",
        "        org_width,\n",
        "        scale=(0.08, 1.0),\n",
        "        ratio=(0.75, 1.3333333333333333),\n",
        "    ):\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.scale = scale\n",
        "        self.ratio = ratio\n",
        "        self.beforeheight = org_height\n",
        "        self.beforewidth = org_width\n",
        "    \n",
        "    @staticmethod\n",
        "    def get_random_crop_coords(height, width, crop_height, crop_width, h_start, w_start):\n",
        "        x1 = int((height - crop_height) * h_start)\n",
        "        x2 = x1 + crop_height\n",
        "        y1 = int((width - crop_width) * w_start)\n",
        "        y2 = y1 + crop_width\n",
        "        return x1, y1, x2, y2\n",
        "    \n",
        "    def __call__(self, img):\n",
        "\n",
        "        \n",
        "        area = img.shape[0] * img.shape[1]\n",
        "        #print(img.shape[0], img.shape[1])\n",
        "        for _attempt in range(10):\n",
        "            target_area = random.uniform(*self.scale) * area\n",
        "            log_ratio = (math.log(self.ratio[0]), math.log(self.ratio[1]))\n",
        "            aspect_ratio = math.exp(random.uniform(*log_ratio))\n",
        "\n",
        "            w = int(round(math.sqrt(target_area * aspect_ratio)))  # skipcq: PTC-W0028\n",
        "            h = int(round(math.sqrt(target_area / aspect_ratio)))  # skipcq: PTC-W0028\n",
        "            #print(w, h)\n",
        "            if 0 < w <= img.shape[1] and 0 < h <= img.shape[0]:\n",
        "                i = random.randint(0, img.shape[0] - h)\n",
        "                j = random.randint(0, img.shape[1] - w)\n",
        "                h_start = i * 1.0 / (img.shape[0] - h + 1e-10)\n",
        "                w_start = j * 1.0 / (img.shape[1] - w + 1e-10)\n",
        "                #print(h, w)\n",
        "                x1, y1, x2, y2 = self.get_random_crop_coords(self.beforeheight, self.beforewidth, h, w, h_start, w_start)\n",
        "                #print(h, w)\n",
        "                #print(x1, y1, x2, y2)\n",
        "                #print(x1, y1, x2, y2)\n",
        "                img = img[x1:x2, y1:y2, :]\n",
        "                img = tf.image.resize(img, (self.height, self.width))\n",
        "                return tf.cast(img, dtype=tf.uint8)\n",
        "\n",
        "        # Fallback to central crop\n",
        "        #print('central gogo')\n",
        "        in_ratio = img.shape[1] / img.shape[0]\n",
        "        if in_ratio < min(self.ratio):\n",
        "            w = img.shape[1]\n",
        "            h = int(round(w / min(self.ratio)))\n",
        "        elif in_ratio > max(self.ratio):\n",
        "            h = img.shape[0]\n",
        "            w = int(round(h * max(self.ratio)))\n",
        "        else:  # whole image\n",
        "            w = img.shape[1]\n",
        "            h = img.shape[0]\n",
        "        i = (img.shape[0] - h) // 2\n",
        "        j = (img.shape[1] - w) // 2\n",
        "        x1, y1, x2, y2 = self.get_random_crop_coords(self.beforeheight, self.beforewidth, h, w, i, j)\n",
        "        img = img[x1:x2, y1:y2, :]\n",
        "        img = tf.image.resize(img, (self.height, self.width))\n",
        "        return tf.cast(img, dtype=tf.uint8)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDDsiU6PE6Zm"
      },
      "source": [
        "class Normalize:\n",
        "    \"\"\"Divide pixel values by 255 = 2**8 - 1, subtract mean per channel and divide by std per channel.\n",
        "    Args:\n",
        "        mean (float, list of float): mean values\n",
        "        std  (float, list of float): std values\n",
        "        max_pixel_value (float): maximum possible pixel value\n",
        "    Targets:\n",
        "        image\n",
        "    Image types:\n",
        "        uint8, float32\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, mean=CFG.MEAN, std=CFG.STD, max_pixel_value=255.0, always_apply=False, p=1.0\n",
        "    ):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "        self.max_pixel_value = max_pixel_value\n",
        "\n",
        "\n",
        "    \n",
        "    def normalize_image(self, img, mean, std, max_pixel_value=255.0):\n",
        "        mean = tf.convert_to_tensor(mean, dtype=tf.float32)\n",
        "        mean = mean * max_pixel_value\n",
        "\n",
        "        std = tf.convert_to_tensor(std, dtype=tf.float32)\n",
        "        std = std * max_pixel_value\n",
        "\n",
        "        denominator = tf.math.reciprocal(std)\n",
        "\n",
        "        #print('before cast', img)\n",
        "        img = tf.cast(img, dtype = tf.float32)\n",
        "        #print('after cast', img)\n",
        "        #img = img - mean\n",
        "        #img = img * denominator\n",
        "        img = img / 255.\n",
        "        return img\n",
        "\n",
        "    def __call__(self, img):\n",
        "        return self.normalize_image(img, self.mean, self.std)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ivUmviTzRd6"
      },
      "source": [
        "class CoarseDropout:\n",
        "  def __init__(self, max_holes, size=0.06):\n",
        "    self.size = size\n",
        "    self.max_holes = max_holes\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def __call__(self, image):\n",
        "      #holes = []\n",
        "      P = random.uniform(0,1)\n",
        "      height = image.shape[0]\n",
        "      width = image.shape[1]\n",
        "      for _n in range(self.max_holes):\n",
        "          hole_height = height * self.size * P\n",
        "          hole_width = width * self.size * P\n",
        "          hole_height = int(hole_height)\n",
        "          hole_width = int(hole_width)\n",
        "          y1 = random.randint(0, height - hole_height)\n",
        "          x1 = random.randint(0, width- hole_width)\n",
        "          y2 = y1 + hole_height\n",
        "          x2 = x1 + hole_width\n",
        "          #holes.append((y1, x1, y2, x2))\n",
        "        \n",
        "          one = image[y1:y2,0:x1,:]\n",
        "          two = tf.zeros([y2-y1,x2-x1,3], dtype=tf.uint8) \n",
        "          three = image[y1:y2,x2:width,:]\n",
        "          middle = tf.concat([one,two,three],axis=1)\n",
        "          image = tf.concat([image[0:y1,:,:],middle,image[y2:height,:,:]],axis=0)\n",
        "      \n",
        "          \n",
        "      image = tf.cast(image, dtype=tf.uint8)\n",
        "      return image"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qtmKcRRnLJS"
      },
      "source": [
        "def augment_list():\n",
        " \n",
        "  l = [  #(Identity, 0, 1),\n",
        "        #(AutoContrast, 0, 1),\n",
        "        #(Equalize, 0, 1),\n",
        "        (Rotate, -20, 20),\n",
        "        #(Posterize, 0, 4),\n",
        "        #(Solarize, 0, 256),\n",
        "        #(Color, 0.1, 1.9),\n",
        "        (Contrast, 0.1, 1.9),\n",
        "        (Brightness, 0.1, 1.9),\n",
        "        #(Sharpness, 0.1, 1.9),\n",
        "        (ShearX, -0.1, 0.1),\n",
        "        (ShearY, -0.1, 0.1),\n",
        "        (TranslateX, -CFG.OBJ_WIDTH * 0.0625 * 30 / CFG.RANDAUG_MAGNITUDE, CFG.OBJ_WIDTH * 0.0625 * 30 / CFG.RANDAUG_MAGNITUDE),\n",
        "        (TranslateY, -CFG.OBJ_HEIGHT * 0.0625 * 30 / CFG.RANDAUG_MAGNITUDE, CFG.OBJ_HEIGHT * 0.0625 * 30 / CFG.RANDAUG_MAGNITUDE),\n",
        "    ]\n",
        "  return l"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tzakJMqhjAY"
      },
      "source": [
        "def augment_list_tta():\n",
        " \n",
        "  l = [  #(Identity, 0, 1),\n",
        "        #(AutoContrast, 0, 1),\n",
        "        #(Equalize, 0, 1),\n",
        "        #(Rotate, -15, 15),\n",
        "        #(Posterize, 0, 4),\n",
        "        #(Solarize, 0, 256),\n",
        "        #(Color, 0.1, 1.9),\n",
        "        #(Contrast, 0.1, 1.9),\n",
        "        (Brightness, 0.1, 1.9),\n",
        "        #(Sharpness, 0.1, 1.9),\n",
        "        #(ShearX, -0.1, 0.1),\n",
        "        #(ShearY, -0.1, 0.1),\n",
        "        #(TranslateX, -CFG.OBJ_WIDTH * 0.0625 * 30 / CFG.RANDAUG_MAGNITUDE, CFG.OBJ_WIDTH * 0.0625 * 30 / CFG.RANDAUG_MAGNITUDE),\n",
        "        #(TranslateY, -CFG.OBJ_HEIGHT * 0.0625 * 30 / CFG.RANDAUG_MAGNITUDE, CFG.OBJ_HEIGHT * 0.0625 * 30 / CFG.RANDAUG_MAGNITUDE),\n",
        "    ]\n",
        "  return l"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYyHDbDpnfBk"
      },
      "source": [
        "import random\n",
        "class RandAugment:\n",
        "    def __init__(self, n, m):\n",
        "        self.n = n\n",
        "        self.m = m      # [0, 30]\n",
        "        self.augment_list = augment_list()\n",
        " \n",
        "    def __call__(self, img):\n",
        "        ops = random.choices(self.augment_list, k=self.n)\n",
        "        for op, minval, maxval in ops:\n",
        "            val = (float(self.m) / 30) * float(maxval - minval) + minval\n",
        "            img = op(img, val)\n",
        " \n",
        " \n",
        "        return img"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykrpFND_rQ-p"
      },
      "source": [
        "import random\n",
        "class RandAugmentTTA:\n",
        "    def __init__(self, n, m):\n",
        "        self.n = n\n",
        "        self.m = m      # [0, 30]\n",
        "        self.augment_list = augment_list_tta()\n",
        " \n",
        "    def __call__(self, img):\n",
        "        ops = random.choices(self.augment_list, k=self.n)\n",
        "        for op, minval, maxval in ops:\n",
        "            val = (float(self.m) / 30) * float(maxval - minval) + minval\n",
        "            img = op(img, val)\n",
        " \n",
        " \n",
        "        return img"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iB4CyRdNAKS4"
      },
      "source": [
        "randomaugtta = RandAugmentTTA(CFG.RANDAUG_NUM,CFG.RANDAUG_MAGNITUDE)\n",
        "randaugtta = RandAugmentTTA(CFG.RANDAUG_NUM,CFG.RANDAUG_MAGNITUDE)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oqGkm0mLoW0"
      },
      "source": [
        "def cutmix(image, label, PROBABILITY = 1.0):\n",
        "    # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3]\n",
        "    # output - a batch of images with cutmix applied\n",
        "    #print(image.shape, label.shape)\n",
        "    DIM1 = CFG.OBJ_HEIGHT\n",
        "    DIM2 = CFG.OBJ_WIDTH\n",
        "    CLASSES = CFG.NUMBER_OF_CLASSES\n",
        "    AUG_BATCH = CFG.BATCH_SIZE\n",
        "    cutmix_start = 0.0\n",
        "    imgs = []; labs = []\n",
        "    \n",
        "    image = tf.image.resize(image, size=(DIM1, DIM2))\n",
        "    image = tf.cast(image, dtype=tf.float32)\n",
        "    for j in range(AUG_BATCH):\n",
        "        # DO CUTMIX WITH PROBABILITY DEFINED ABOVE\n",
        "        P = tf.cast( tf.random.uniform([],cutmix_start,1)<=PROBABILITY, tf.int32)\n",
        "        # CHOOSE RANDOM IMAGE TO CUTMIX WITH\n",
        "        k = tf.cast( tf.random.uniform([],0,AUG_BATCH),tf.int32)\n",
        "        # CHOOSE RANDOM LOCATION\n",
        "        x = tf.cast( tf.random.uniform([],0,DIM2),tf.int32)\n",
        "        y = tf.cast( tf.random.uniform([],0,DIM1),tf.int32)\n",
        "        a = tf.cast(np.random.beta(0.3,0.3), dtype=tf.float32)\n",
        "        b = tf.cast(np.random.beta(0.3,0.3), dtype=tf.float32) # this is beta dist with alpha=1.0\n",
        "        WIDTH = tf.cast( DIM2 * tf.math.sqrt(1-a),tf.int32) * P\n",
        "        HEIGHT = tf.cast( DIM1 * tf.math.sqrt(1-b), tf.int32) * P\n",
        "        ya = tf.math.maximum(0,y-HEIGHT//2)\n",
        "        yb = tf.math.minimum(DIM1,y+HEIGHT//2)\n",
        "        xa = tf.math.maximum(0,x-WIDTH//2)\n",
        "        xb = tf.math.minimum(DIM2,x+WIDTH//2)\n",
        "        # MAKE CUTMIX IMAGE\n",
        "        one = image[j,ya:yb,0:xa,:]\n",
        "        two = image[k,ya:yb,xa:xb,:]\n",
        "        three = image[j,ya:yb,xb:DIM2,:]\n",
        "        middle = tf.concat([one,two,three],axis=1)\n",
        "        cutmix_img = tf.concat([image[j,0:ya,:,:],middle,image[j,yb:DIM1,:,:]],axis=0)\n",
        "        p_flip = tf.random.uniform([], 0, 1, dtype=tf.float32)\n",
        "        p_v_flip = tf.random.uniform([], 0, 1, dtype=tf.float32)\n",
        "        p_transpose = tf.random.uniform([], 0, 1, dtype=tf.float32)\n",
        "        if p_flip > 0.5:\n",
        "            cutmix_img = tf.image.flip_left_right(cutmix_img)\n",
        "        if p_v_flip > 0.5:\n",
        "            cutmix_img = tf.image.flip_up_down(cutmix_img)\n",
        "        if p_transpose > 0.5:\n",
        "            cutmix_img = tf.image.transpose(cutmix_img)\n",
        "        #cutmix_img = Normalize(CFG.MEAN, CFG.STD)(cutmix_img)\n",
        "        #cutmix_img = tf.image.resize(cutmix_img, size=(CFG.OBJ_HEIGHT, CFG.OBJ_WIDTH))\n",
        "        #mixup_image = (1-a)*img1 + a*img2\n",
        "        cutmix_img = Normalize(CFG.MEAN, CFG.STD)(cutmix_img)\n",
        "        #mixup_image = tf.image.resize(mixup_image, size=(CFG.OBJ_HEIGHT, CFG.OBJ_WIDTH))\n",
        "        #imgs.append(mixup_image)\n",
        "        imgs.append(cutmix_img)\n",
        "        # MAKE CUTMIX LABEL\n",
        "        a = tf.cast(WIDTH*HEIGHT/DIM1/DIM2,tf.float32)\n",
        "        if len(label.shape)==1:\n",
        "            lab1 = tf.one_hot(label[j],CLASSES)\n",
        "            lab2 = tf.one_hot(label[k],CLASSES)\n",
        "        else:\n",
        "            lab1 = label[j,]\n",
        "            lab2 = label[k,]\n",
        "        labs.append((1-a)*lab1 + a*lab2)\n",
        "            \n",
        "    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?)\n",
        "    image2 = tf.reshape(tf.stack(imgs),(AUG_BATCH,CFG.OBJ_HEIGHT,CFG.OBJ_WIDTH,3))\n",
        "    label2 = tf.reshape(tf.stack(labs),(AUG_BATCH,CLASSES))\n",
        "    return image2,label2\n",
        " \n",
        " \n",
        "def mixup(image, label, PROBABILITY = 1.0):\n",
        "    # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3]\n",
        "    # output - a batch of images with mixup applied\n",
        "    AUG_BATCH = CFG.BATCH_SIZE\n",
        "    DIM1 = CFG.OBJ_HEIGHT\n",
        "    DIM2 = CFG.OBJ_WIDTH\n",
        "    CLASSES = CFG.NUMBER_OF_CLASSES\n",
        "    \n",
        "    imgs = []; labs = []\n",
        "    image = tf.image.resize(image, size=(DIM1, DIM2))\n",
        "    image = tf.cast(image, dtype=tf.float32)\n",
        "    for j in range(AUG_BATCH):\n",
        "        # DO MIXUP WITH PROBABILITY DEFINED ABOVE\n",
        "        P = tf.cast( tf.random.uniform([],0,1)<=PROBABILITY, tf.float32)\n",
        "        # CHOOSE RANDOM\n",
        "        k = tf.cast( tf.random.uniform([],0,AUG_BATCH),tf.int32)\n",
        "        a = tf.cast(np.random.beta(0.3,0.3), dtype=tf.float32)*P # this is beta dist with alpha=1.0\n",
        "        # MAKE MIXUP IMAGE\n",
        "        img1 = image[j,]\n",
        "        img2 = image[k,]\n",
        "        #mixup_image = (1-0.5)*img1 + 0.5*img2\n",
        "        mixup_image = (1-a)*img1 + a*img2\n",
        "        p_flip = tf.random.uniform([], 0, 1, dtype=tf.float32)\n",
        "        #p_v_flip = tf.random.uniform([], 0, 1, dtype=tf.float32)\n",
        "        #p_transpose = tf.random.uniform([], 0, 1, dtype=tf.float32)\n",
        "        if p_flip > 0.5:\n",
        "            mixup_image = tf.image.flip_left_right(mixup_image)\n",
        "        #if p_v_flip > 0.5:\n",
        "        #    mixup_image = tf.image.flip_up_down(mixup_image)\n",
        "        #if p_transpose > 0.5:\n",
        "        #    mixup_image = tf.image.transpose(mixup_image)\n",
        "        mixup_image = Normalize(CFG.MEAN, CFG.STD)(mixup_image)\n",
        "        #mixup_image = tf.image.resize(mixup_image, size=(CFG.OBJ_HEIGHT, CFG.OBJ_WIDTH))\n",
        "        imgs.append(mixup_image)\n",
        "        # MAKE CUTMIX LABEL\n",
        "        if len(label.shape)==1:\n",
        "            lab1 = tf.one_hot(label[j],CLASSES)\n",
        "            lab2 = tf.one_hot(label[k],CLASSES)\n",
        "        else:\n",
        "            lab1 = label[j,]\n",
        "            lab2 = label[k,]\n",
        "        labs.append((1-a)*lab1 + a*lab2)\n",
        "            \n",
        "    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?)\n",
        "    image2 = tf.reshape(tf.stack(imgs),(AUG_BATCH,CFG.OBJ_HEIGHT,CFG.OBJ_WIDTH,3))\n",
        "    label2 = tf.reshape(tf.stack(labs),(AUG_BATCH,CLASSES))\n",
        "    return image2,label2"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEAxFwBse-yv"
      },
      "source": [
        "def real_data_augment(image, label):\n",
        "    k = tf.random.uniform([], 0, 1, dtype=tf.float32)\n",
        "    if k < 0.5:\n",
        "        image2, label2 = mixup(image, label)\n",
        "    #elif (k >= -2) and (k < 0):\n",
        "    #    image2, label2 = cutmix(image, label)\n",
        "    else:\n",
        "        image2, label2 = data_augment(image, label)\n",
        " \n",
        "    return image2, label2 \n",
        " \n",
        "def prep_for_val(image, label):\n",
        "    \n",
        "    DIM1 = CFG.OBJ_HEIGHT\n",
        "    DIM2 = CFG.OBJ_WIDTH\n",
        "    CLASSES = CFG.NUMBER_OF_CLASSES\n",
        "    AUG_BATCH = CFG.AUG_BATCH\n",
        "    imgs = []; labs = []\n",
        "    #randaug = RandAugment(CFG.RANDAUG_NUM,CFG.RANDAUG_MAGNITUDE)\n",
        "    #randaug = RandAugment(3, 12)\n",
        "    normalize = Normalize(CFG.MEAN, CFG.STD)\n",
        "    #coarse = CoarseDropout(30)\n",
        "    #randomcrop = RandomResizedCrop(CFG.OBJ_HEIGHT, CFG.OBJ_WIDTH, CFG.HEIGHT, CFG.WIDTH, scale=(0.85, 1.0))\n",
        " \n",
        "    #P_NORMAL_OR_MIX = tf.random.uniform([],0,1,dtype=tf.float32)\n",
        " \n",
        " \n",
        "    for j in range(AUG_BATCH):        \n",
        "            img = image[j,:,:,:]\n",
        " \n",
        "            img = tf.image.resize(img, [DIM1, DIM2])\n",
        "            img = normalize(img)\n",
        "            imgs.append(img)\n",
        " \n",
        "       \n",
        "            lab1 = label[j,]\n",
        "            labs.append(lab1)\n",
        " \n",
        "    image2 = tf.reshape(tf.stack(imgs),[AUG_BATCH, DIM1,DIM2,3])\n",
        "    label2 = tf.reshape(tf.stack(labs),(AUG_BATCH,CLASSES))\n",
        " \n",
        "    return image2,label2\n",
        " \n",
        " \n",
        "def data_augment(image, label):\n",
        " \n",
        " \n",
        "    DIM1 = CFG.OBJ_HEIGHT\n",
        "    DIM2 = CFG.OBJ_WIDTH\n",
        "    CLASSES = CFG.NUMBER_OF_CLASSES\n",
        "    AUG_BATCH = CFG.AUG_BATCH\n",
        "    imgs = []; labs = []\n",
        "    randaug = RandAugment(CFG.RANDAUG_NUM,CFG.RANDAUG_MAGNITUDE)\n",
        "    #randaug = RandAugment(3, 12)\n",
        "    normalize = Normalize(CFG.MEAN, CFG.STD)\n",
        "    coarse = CoarseDropout(1)\n",
        "    randomcrop = RandomResizedCrop(CFG.OBJ_HEIGHT, CFG.OBJ_WIDTH, CFG.HEIGHT, CFG.WIDTH, scale=(0.9, 1.1))\n",
        " \n",
        "    P_NORMAL_OR_MIX = tf.random.uniform([],0,1,dtype=tf.float32)\n",
        " \n",
        " \n",
        "    for j in range(AUG_BATCH):        \n",
        "            img = image[j,:,:,:]\n",
        " \n",
        "            p_flip = tf.random.uniform([], 0, 1, dtype=tf.float32)\n",
        "            p_v_flip = tf.random.uniform([], 0, 1, dtype=tf.float32)\n",
        "            p_transpose = tf.random.uniform([], 0, 1, dtype=tf.float32)\n",
        " \n",
        "            img = randomcrop(img)\n",
        " \n",
        "            if p_flip >= 0.5:\n",
        "                img = tf.image.flip_left_right(img)\n",
        "            if p_v_flip >= 0.5:\n",
        "                img = tf.image.flip_up_down(img)\n",
        "            #if p_transpose >= 0.5:\n",
        "            #    if CFG.OBJ_HEIGHT == CFG.OBJ_WIDTH:\n",
        "            #        img = tf.image.transpose(img)\n",
        " \n",
        "            img = coarse(img)\n",
        "            img = randaug(img)\n",
        "            #img = img/255.\n",
        "            img = tf.cast(img, tf.float32) / 255.\n",
        "            imgs.append(img)\n",
        " \n",
        "       \n",
        "            lab1 = label[j,]\n",
        "            labs.append(lab1)\n",
        " \n",
        "    image2 = tf.reshape(tf.stack(imgs),[AUG_BATCH, DIM1,DIM2,3])\n",
        "    label2 = tf.reshape(tf.stack(labs),(AUG_BATCH,CLASSES))\n",
        " \n",
        "    return image2,label2"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECrHNVz2S4Dk",
        "outputId": "f6fa5111-8719-4556-ab10-96994e95cace"
      },
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36026"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bI8RTuBuSEpg"
      },
      "source": [
        "#def decode_tr_image(image_data):\n",
        "#    image = tf.image.decode_jpeg(image_data, channels=3)\n",
        "#    \n",
        "#    image = tf.reshape(image, [CFG.HEIGHT, CFG.WIDTH, 3])\n",
        "#    \n",
        "#    return image\n",
        "def decode_tr_image(image_data, label):\n",
        "    \"\"\"\n",
        "        1. Decode a JPEG-encoded image to a uint8 tensor.\n",
        "        2. Cast tensor to float and normalizes (range between 0 and 1).\n",
        "        3. Resize and reshape images to the expected size.\n",
        "    \"\"\"\n",
        "    ################################\n",
        "    ############ Image  ############\n",
        "    ################################\n",
        "    image = tf.image.decode_jpeg(image_data, channels=3)\n",
        "    image = tf.reshape(image, [CFG.HEIGHT, CFG.WIDTH, 3])\n",
        "    image = tf.image.resize(image, size=(CFG.OBJ_HEIGHT, CFG.OBJ_WIDTH))\n",
        "    #normalize = Normalize(CFG.MEAN, CFG.STD)\n",
        "    #image = normalize(image)\n",
        "    image = tf.cast(image, tf.float32) / 255.\n",
        "    image = tf.reshape(image, [CFG.OBJ_HEIGHT, CFG.OBJ_WIDTH, 3])\n",
        "    ################################\n",
        "    ############ Mask  #############\n",
        "    ################################\n",
        "    label['segg'] = tf.image.decode_jpeg(label['segg'], channels=1)\n",
        "    label['segg'] = tf.cast(label['segg'], tf.float32) / 255.0\n",
        "    label['segg'] = tf.reshape(label['segg'], [CFG.MASK_OBJ_HEIGHT, CFG.MASK_OBJ_WIDTH, 1])\n",
        "    label['clss'] = tf.reshape(label['clss'], [CFG.NUMBER_OF_CLASSES])\n",
        "\n",
        "\n",
        "    ########################################\n",
        "    ############ Augmentation  #############\n",
        "    ########################################\n",
        "    \n",
        "    return image, label\n",
        " \n",
        "def decode_val_image(image_data, label):\n",
        "    \"\"\"\n",
        "        1. Decode a JPEG-encoded image to a uint8 tensor.\n",
        "        2. Cast tensor to float and normalizes (range between 0 and 1).\n",
        "        3. Resize and reshape images to the expected size.\n",
        "    \"\"\"\n",
        "    ################################\n",
        "    ############ Image  ############\n",
        "    ################################\n",
        "    image = tf.image.decode_jpeg(image_data, channels=3)\n",
        "    image = tf.reshape(image, [CFG.HEIGHT, CFG.WIDTH, 3])\n",
        "    image = tf.image.resize(image, size=(CFG.OBJ_HEIGHT, CFG.OBJ_WIDTH))\n",
        "    #normalize = Normalize(CFG.MEAN, CFG.STD)\n",
        "    #image = normalize(image)\n",
        "    image = tf.cast(image, tf.float32) / 255.\n",
        "    image = tf.reshape(image, [CFG.OBJ_HEIGHT, CFG.OBJ_WIDTH, 3])\n",
        "    ################################\n",
        "    ############ Mask  #############\n",
        "    ################################\n",
        "    label['segg'] = tf.image.decode_jpeg(label['segg'], channels=1)\n",
        "    label['segg'] = tf.cast(label['segg'], tf.float32) / 255.0\n",
        "    label['segg'] = tf.reshape(label['segg'], [CFG.MASK_OBJ_HEIGHT, CFG.MASK_OBJ_WIDTH, 1])\n",
        "    label['clss'] = tf.reshape(label['clss'], [CFG.NUMBER_OF_CLASSES])\n",
        "    return image, label\n",
        "\n",
        "def decode_test_image(image_data, label):\n",
        "    \"\"\"\n",
        "        1. Decode a JPEG-encoded image to a uint8 tensor.\n",
        "        2. Cast tensor to float and normalizes (range between 0 and 1).\n",
        "        3. Resize and reshape images to the expected size.\n",
        "    \"\"\"\n",
        "    image = tf.image.decode_jpeg(image_data, channels=3)\n",
        "    image = tf.reshape(image, [CFG.HEIGHT, CFG.WIDTH, 3])\n",
        "    image = tf.image.resize(image, size=(CFG.OBJ_HEIGHT, CFG.OBJ_WIDTH))\n",
        "    normalize = Normalize(CFG.MEAN, CFG.STD)\n",
        "    image = normalize(image)\n",
        "    #image = tf.cast(image, tf.float32) / 255.\n",
        "    \n",
        "    image = tf.reshape(image, [CFG.OBJ_HEIGHT, CFG.OBJ_WIDTH, 3])\n",
        "    return image, label\n",
        "\n",
        "def decode_just_test_image(image_data):\n",
        "    \"\"\"\n",
        "        1. Decode a JPEG-encoded image to a uint8 tensor.\n",
        "        2. Cast tensor to float and normalizes (range between 0 and 1).\n",
        "        3. Resize and reshape images to the expected size.\n",
        "    \"\"\"\n",
        "    image = tf.image.decode_jpeg(image_data, channels=3)\n",
        "    image = tf.reshape(image, [CFG.HEIGHT, CFG.WIDTH, 3])\n",
        "    image = tf.image.resize(image, size=(CFG.OBJ_HEIGHT, CFG.OBJ_WIDTH))\n",
        "    normalize = Normalize(CFG.MEAN, CFG.STD)\n",
        "    image = normalize(image)\n",
        "    #image = tf.cast(image, tf.float32) / 255.\n",
        "    \n",
        "    image = tf.reshape(image, [CFG.OBJ_HEIGHT, CFG.OBJ_WIDTH, 3])\n",
        "    return image\n",
        " \n",
        "def decode_val_image_for_tta(image_data):\n",
        "    \"\"\"\n",
        "        1. Decode a JPEG-encoded image to a uint8 tensor.\n",
        "        2. Cast tensor to float and normalizes (range between 0 and 1).\n",
        "        3. Resize and reshape images to the expected size.\n",
        "    \"\"\"\n",
        "    randomcrop = RandomResizedCrop(CFG.OBJ_HEIGHT, CFG.OBJ_WIDTH, CFG.HEIGHT, CFG.WIDTH, scale=(0.99,1.0))\n",
        "    randaug = RandAugmentTTA(3, 12)\n",
        "    normalize = Normalize(CFG.MEAN, CFG.STD)\n",
        "    p_flip = tf.random.uniform([], 0, 1, dtype=tf.float32)\n",
        "    p_v_flip = tf.random.uniform([], 0, 1, dtype=tf.float32)\n",
        "    #p_transpose = tf.random.uniform([], 0, 1, dtype=tf.float32)\n",
        "    image = tf.image.decode_jpeg(image_data, channels=3)\n",
        "    image = tf.reshape(image, [CFG.HEIGHT, CFG.WIDTH, 3])\n",
        "    image = tf.image.resize(image, size=(CFG.OBJ_HEIGHT, CFG.OBJ_WIDTH))\n",
        "    #image = randomcrop(image)\n",
        "    if p_flip > 0.5:\n",
        "        image = tf.image.flip_left_right(image)\n",
        "    if p_v_flip > 0.5:\n",
        "        image = tf.image.flip_up_down(image)\n",
        "    #if CFG.OBJ_HEIGHT == CFG.OBJ_WIDTH:\n",
        "    #    if p_transpose > 0.5:\n",
        "    #        image = tf.image.transpose(image)\n",
        "    image = randaugtta(image)\n",
        "    #image = normalize(image)\n",
        "    image = tf.cast(image, tf.float32) / 255.\n",
        "    image = tf.reshape(image, [CFG.OBJ_HEIGHT, CFG.OBJ_WIDTH, 3])\n",
        "    return image\n",
        " \n",
        "def read_tfrecord(example, labeled=True):\n",
        "    \"\"\"\n",
        "        1. Parse data based on the 'TFREC_FORMAT' map.\n",
        "        2. Decode image.\n",
        "        3. If 'labeled' returns (image, label) if not (image, name).\n",
        "    \"\"\"\n",
        "    if labeled:\n",
        "        TFREC_FORMAT = {\n",
        "            'image': tf.io.FixedLenFeature([], tf.string), \n",
        "            'target': tf.io.FixedLenFeature([], tf.int64),\n",
        "            'mask' : tf.io.FixedLenFeature([], tf.string) \n",
        "        }\n",
        "        example = tf.io.parse_single_example(example, TFREC_FORMAT)\n",
        "        return example['image'], {'clss':tf.one_hot(example['target'], depth=CFG.NUMBER_OF_CLASSES),\n",
        "                                  'segg':example['mask']}\n",
        "    else:\n",
        "        TFREC_FORMAT = {\n",
        "            'image': tf.io.FixedLenFeature([], tf.string), \n",
        "            'image_id': tf.io.FixedLenFeature([], tf.string), \n",
        "        }\n",
        "        example = tf.io.parse_single_example(example, TFREC_FORMAT)\n",
        "        return example['image'], example['image_id']\n",
        "\n",
        "def read_test_tfrecord(example):\n",
        "\n",
        "        TFREC_FORMAT = {\n",
        "            'image': tf.io.FixedLenFeature([], tf.string), \n",
        "            'image_id': tf.io.FixedLenFeature([], tf.string), \n",
        "        }\n",
        "        example = tf.io.parse_single_example(example, TFREC_FORMAT)\n",
        "        return example['image'], example['image_id']\n",
        "\n",
        "def read_test_image_tfrecord(example):\n",
        "\n",
        "        TFREC_FORMAT = {\n",
        "            'image': tf.io.FixedLenFeature([], tf.string), \n",
        "            'image_id': tf.io.FixedLenFeature([], tf.string), \n",
        "        }\n",
        "        example = tf.io.parse_single_example(example, TFREC_FORMAT)\n",
        "        return example['image']\n",
        " \n",
        "def load_dataset(filenames, validation, labeled=True, ordered=False, ):\n",
        "    \"\"\"\n",
        "        Create a Tensorflow dataset from TFRecords.\n",
        "    \"\"\"\n",
        "    ignore_order = tf.data.Options()\n",
        "    if not ordered:\n",
        "        ignore_order.experimental_deterministic = False\n",
        " \n",
        "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n",
        "    dataset = dataset.with_options(ignore_order)\n",
        "    if validation == False:\n",
        " \n",
        "        dataset = dataset.map(lambda x: read_tfrecord(x, labeled=labeled, validation = False), num_parallel_calls=AUTO)\n",
        "    else:\n",
        "        dataset = dataset.map(lambda x: read_tfrecord(x, labeled=labeled, validation = True), num_parallel_calls=AUTO)\n",
        "    return dataset\n",
        " \n",
        "def get_dataset(FILENAMES, labeled=True, ordered=False, repeated=False, augment=False, validation=False):\n",
        "    \"\"\"\n",
        "        Return a Tensorflow dataset ready for training or inference.\n",
        "    \"\"\"\n",
        "    dataset = tf.data.TFRecordDataset(FILENAMES, num_parallel_reads = AUTO)\n",
        "    dataset = dataset.cache()\n",
        "    if repeated:\n",
        "        dataset = dataset.repeat()\n",
        "    \n",
        "    if not ordered:\n",
        "        dataset = dataset.shuffle(1024*8)\n",
        "        opt = tf.data.Options()\n",
        "        opt.experimental_deterministic = False\n",
        "        dataset = dataset.with_options(opt)\n",
        "    \n",
        "    if (labeled == True):\n",
        "        dataset = dataset.map(lambda example : read_tfrecord(example, labeled=True), num_parallel_calls = AUTO)\n",
        "    else:\n",
        "        dataset = dataset.map(lambda example : read_tfrecord(example, labeled=False), num_parallel_calls = AUTO)\n",
        "    \n",
        "    if (validation == True) and (labeled == False):\n",
        "        pass\n",
        "    elif (validation == False) and (labeled == True):\n",
        "        dataset = dataset.map(lambda image, label : decode_tr_image(image, label), num_parallel_calls = AUTO)\n",
        "    else:\n",
        "        dataset = dataset.map(lambda image, label : decode_val_image(image, label), num_parallel_calls = AUTO)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    \n",
        "    \n",
        "    \n",
        "    if augment:\n",
        "            dataset = dataset.map(real_data_augment, num_parallel_calls=AUTO)\n",
        "    #else:\n",
        "    #        dataset = dataset.map(prep_for_val, num_parallel_calls=AUTO)\n",
        "    \n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "    return dataset\n",
        " \n",
        "def get_test_dataset(FILENAMES, return_image_name=False):\n",
        "    \"\"\"\n",
        "        Return a Tensorflow dataset ready for training or inference.\n",
        "    \"\"\"\n",
        "    dataset = tf.data.TFRecordDataset(FILENAMES, num_parallel_reads = AUTO)\n",
        "    dataset = dataset.cache()   \n",
        "    \n",
        "    \n",
        "    if return_image_name:\n",
        "        dataset = dataset.map(lambda example : read_test_tfrecord(example), num_parallel_calls = AUTO)\n",
        "        dataset = dataset.map(lambda image, label : decode_test_image(image, label), num_parallel_calls = AUTO)\n",
        "    else:\n",
        "        dataset = dataset.map(lambda example : read_test_image_tfrecord(example), num_parallel_calls = AUTO)\n",
        "        dataset = dataset.map(lambda image : decode_just_test_image(image), num_parallel_calls = AUTO)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "    return dataset\n",
        " \n",
        "def get_dataset_for_tta(FILENAMES, labeled=True, ordered=False, repeated=False, augment=False, validation=False):\n",
        "    \"\"\"\n",
        "        Return a Tensorflow dataset ready for training or inference.\n",
        "    \"\"\"\n",
        "    dataset = tf.data.TFRecordDataset(FILENAMES, num_parallel_reads = AUTO)\n",
        "    dataset = dataset.cache()\n",
        "    if repeated:\n",
        "        dataset = dataset.repeat()\n",
        "    \n",
        "    if not ordered:\n",
        "        dataset = dataset.shuffle(1024*8)\n",
        "        opt = tf.data.Options()\n",
        "        opt.experimental_deterministic = False\n",
        "        dataset = dataset.with_options(opt)\n",
        "    \n",
        "    if (labeled == True):\n",
        "        dataset = dataset.map(lambda example : read_tfrecord(example, labeled=True), num_parallel_calls = AUTO)\n",
        "    else:\n",
        "        dataset = dataset.map(lambda example : read_tfrecord(example, labeled=False), num_parallel_calls = AUTO)\n",
        "    \n",
        "    if validation == False:\n",
        "        dataset = dataset.map(lambda image, label : (decode_tr_image(image), label), num_parallel_calls = AUTO)\n",
        "    else:\n",
        "        dataset = dataset.map(lambda image, label : (decode_val_image_for_tta(image), label), num_parallel_calls = AUTO)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    \n",
        "    \n",
        "    \n",
        "    if augment:\n",
        "            dataset = dataset.map(real_data_augment, num_parallel_calls=AUTO)\n",
        "    \n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "    return dataset"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEVAoSVZoB-1"
      },
      "source": [
        "def count_data_items(filenames):\n",
        "    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n",
        "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
        "    print(n)\n",
        "    return np.sum(n)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDqawNTsn1vX"
      },
      "source": [
        "from tensorflow_addons.utils.types import FloatTensorLike\n",
        " \n",
        "from typing import Union, Callable, Dict\n",
        "from typeguard import typechecked\n",
        " \n",
        " \n",
        "class CosineDecayRAdam(tfa.optimizers.RectifiedAdam):\n",
        "    def _resource_apply_dense(self, grad, var):\n",
        "        var_dtype = var.dtype.base_dtype\n",
        "        lr_t = self._decayed_lr(var_dtype)\n",
        "        wd_t = self._decayed_wd(var_dtype)\n",
        "        m = self.get_slot(var, \"m\")\n",
        "        v = self.get_slot(var, \"v\")\n",
        "        beta_1_t = self._get_hyper(\"beta_1\", var_dtype)\n",
        "        beta_2_t = self._get_hyper(\"beta_2\", var_dtype)\n",
        "        epsilon_t = tf.convert_to_tensor(self.epsilon, var_dtype)\n",
        "        local_step = tf.cast(self.iterations + 1, var_dtype)\n",
        "        beta_1_power = tf.pow(beta_1_t, local_step)\n",
        "        beta_2_power = tf.pow(beta_2_t, local_step)\n",
        " \n",
        "        if self._initial_total_steps > 0:\n",
        "            total_steps = self._get_hyper(\"total_steps\", var_dtype)\n",
        "            warmup_steps = total_steps * self._get_hyper(\"warmup_proportion\", var_dtype)\n",
        "            min_lr = self._get_hyper(\"min_lr\", var_dtype)\n",
        "            decay_steps = tf.maximum(total_steps - warmup_steps, 1)\n",
        "            decay_rate = (min_lr - lr_t) / decay_steps\n",
        "            pi = tf.constant(3.141592)\n",
        "            cos = tf.math.cos(pi * ((local_step - warmup_steps) / (total_steps - warmup_steps))) + tf.constant(1.)\n",
        "            lr_t = tf.where(\n",
        "                local_step <= warmup_steps,\n",
        "                lr_t * (local_step / warmup_steps),\n",
        "                #lr_t + decay_rate * tf.minimum(local_step - warmup_steps, decay_steps),\n",
        "                min_lr + (lr_t - min_lr) / 2. * cos\n",
        "            )\n",
        " \n",
        "        sma_inf = 2.0 / (1.0 - beta_2_t) - 1.0\n",
        "        sma_t = sma_inf - 2.0 * local_step * beta_2_power / (1.0 - beta_2_power)\n",
        " \n",
        "        m_t = m.assign(\n",
        "            beta_1_t * m + (1.0 - beta_1_t) * grad, use_locking=self._use_locking\n",
        "        )\n",
        "        m_corr_t = m_t / (1.0 - beta_1_power)\n",
        " \n",
        "        v_t = v.assign(\n",
        "            beta_2_t * v + (1.0 - beta_2_t) * tf.square(grad),\n",
        "            use_locking=self._use_locking,\n",
        "        )\n",
        "        if self.amsgrad:\n",
        "            vhat = self.get_slot(var, \"vhat\")\n",
        "            vhat_t = vhat.assign(tf.maximum(vhat, v_t), use_locking=self._use_locking)\n",
        "            v_corr_t = tf.sqrt(vhat_t / (1.0 - beta_2_power))\n",
        "        else:\n",
        "            vhat_t = None\n",
        "            v_corr_t = tf.sqrt(v_t / (1.0 - beta_2_power))\n",
        " \n",
        "        r_t = tf.sqrt(\n",
        "            (sma_t - 4.0)\n",
        "            / (sma_inf - 4.0)\n",
        "            * (sma_t - 2.0)\n",
        "            / (sma_inf - 2.0)\n",
        "            * sma_inf\n",
        "            / sma_t\n",
        "        )\n",
        " \n",
        "        sma_threshold = self._get_hyper(\"sma_threshold\", var_dtype)\n",
        "        var_t = tf.where(\n",
        "            sma_t >= sma_threshold, r_t * m_corr_t / (v_corr_t + epsilon_t), m_corr_t\n",
        "        )\n",
        " \n",
        "        if self._has_weight_decay:\n",
        "            var_t += wd_t * var\n",
        " \n",
        "        var_update = var.assign_sub(lr_t * var_t, use_locking=self._use_locking)\n",
        " \n",
        "        updates = [var_update, m_t, v_t]\n",
        "        if self.amsgrad:\n",
        "            updates.append(vhat_t)\n",
        "        return tf.group(*updates)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grpG6YZ-BP1d"
      },
      "source": [
        "from tensorflow.keras import applications \n",
        "\n",
        "class CovidNet(tf.keras.models.Model):\n",
        "    def __init__(self):\n",
        "        super(CovidNet, self).__init__()\n",
        "        self.base = tf.keras.applications.EfficientNetB7(input_shape=(512, 512, 3),\n",
        "                                                         include_top=False,\n",
        "                                                         weights='imagenet')\n",
        "        # desired model \n",
        "        self.base = tf.keras.models.Model(\n",
        "                [self.base.inputs], \n",
        "                [self.base.output, self.base.get_layer('block5j_add').output]\n",
        "            )\n",
        "        \n",
        "        # tail / head for the classifier \n",
        "        self.tail = tf.keras.Sequential(\n",
        "            [\n",
        "                tf.keras.layers.GlobalAveragePooling2D(),\n",
        "                tf.keras.layers.Dropout(0.5),\n",
        "                tf.keras.layers.BatchNormalization(),\n",
        "                tf.keras.layers.ReLU(),\n",
        "                tf.keras.layers.Dense(4, dtype='float32', activation='softmax'),\n",
        "            ]\n",
        "        )\n",
        "        \n",
        "        # tail / head for the mask \n",
        "        self.msk = tf.keras.Sequential(\n",
        "            [\n",
        "                tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding=\"same\"),\n",
        "                tf.keras.layers.BatchNormalization(),\n",
        "                tf.keras.layers.ReLU(),\n",
        "                tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding=\"same\"),\n",
        "                tf.keras.layers.BatchNormalization(),\n",
        "                tf.keras.layers.ReLU(),\n",
        "                tf.keras.layers.Conv2D(filters=1, kernel_size=(1,1), padding=\"same\", dtype='float32')\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    # feed-forwarding  \n",
        "    def call(self, inputs, training=None, **kwargs):\n",
        "        clss, segg = self.base(inputs)\n",
        "\n",
        "        return {\n",
        "            'clss': self.tail(clss), \n",
        "            'segg': self.msk(segg)\n",
        "        }"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qL84hfUC4x-",
        "outputId": "8515c082-ddb3-4495-e65d-69e3a04b992e"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "model = CovidNet()\n",
        "model.build(input_shape=(None, 512, 512, 3))\n",
        "model.summary()"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"covid_net\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "model (Functional)           [(None, 16, 16, 2560), (N 64097687  \n",
            "_________________________________________________________________\n",
            "sequential (Sequential)      (None, 4)                 20484     \n",
            "_________________________________________________________________\n",
            "sequential_1 (Sequential)    (None, 32, 32, 1)         406913    \n",
            "=================================================================\n",
            "Total params: 64,525,084\n",
            "Trainable params: 64,208,725\n",
            "Non-trainable params: 316,359\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cgT0Cn7E8yS"
      },
      "source": [
        "model.compile(\n",
        "    loss = {\n",
        "        'clss': tf.keras.losses.CategoricalCrossentropy(\n",
        "            label_smoothing=0, from_logits=False),\n",
        "        'segg': tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "    },\n",
        "    \n",
        "    metrics = {\n",
        "        'clss': [\n",
        "            tf.keras.metrics.AUC(curve='ROC', multi_label=True),\n",
        "        ]\n",
        "    },\n",
        "    \n",
        "    optimizer = optimizers.Adam(0.0001)\n",
        ")\n",
        "\n",
        "# list of call backs \n",
        "from tensorflow.keras import callbacks\n",
        "callback_list = [\n",
        "       callbacks.ModelCheckpoint(\n",
        "            filepath='model.{epoch:02d}-{val_loss:.4f}.h5', \n",
        "            save_freq='epoch', verbose=1, monitor='val_loss', \n",
        "            save_weights_only=True, save_best_only=True\n",
        "       )         \n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_qF1Hy6-NKU"
      },
      "source": [
        "inp = tf.keras.layers.Input(shape = (CFG.OBJ_HEIGHT,CFG.OBJ_WIDTH, 3), name = 'inp1')\n",
        "effnet = effnets[NET](weights = 'imagenet', include_top = False, pooling='avg')\n",
        "for layer in effnet.layers:\n",
        "    if 'bn' in layer.name:\n",
        "        layer.trainable = True\n",
        "        \n",
        "x0 = effnet(inp)\n",
        "x0 = tf.keras.layers.Dropout(0.5)(x0)\n",
        "x0 = tf.keras.layers.Dense(64, activation='relu')(x0)\n",
        "x = tf.keras.layers.Dense(CFG.NUMBER_OF_CLASSES, activation='softmax', dtype='float32')(x0)\n",
        "model = tf.keras.models.Model(inputs = inp, outputs = x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YojUScT2975z"
      },
      "source": [
        "inp = tf.keras.layers.Input(shape = (CFG.OBJ_HEIGHT,CFG.OBJ_WIDTH, 3), name = 'inp1')\n",
        "effnet = effnets[NET](weights = 'imagenet', include_top = False, pooling='avg')\n",
        "for layer in effnet.layers:\n",
        "    if 'bn' in layer.name:\n",
        "        layer.trainable = True\n",
        "        \n",
        "x0 = effnet(inp)\n",
        "x0 = tf.keras.layers.Dropout(0.5)(x0)\n",
        "x0 = tf.keras.layers.Dense(64, activation='relu')(x0)\n",
        "x = tf.keras.layers.Dense(CFG.NUMBER_OF_CLASSES, activation='softmax', dtype='float32')(x0)\n",
        " \n",
        "model = tf.keras.models.Model(inputs = inp, outputs = x)\n",
        "opt = CosineDecayRAdam(learning_rate=CFG.LEARNING_RATE, total_steps=int(STEPS_PER_EPOCH*CFG.EPOCHS), warmup_proportion=0.1, min_lr=2e-6)\n",
        "        \n",
        "opt = tfa.optimizers.Lookahead(opt)\n",
        "        #opt =tf.keras.optimizers.Adam(learning_rate=CFG.LEARNING_RATE)\n",
        "model.compile(optimizer = opt,\n",
        "             loss = 'categorical_crossentropy',\n",
        "             metrics = [tf.keras.metrics.AUC(multi_label=True)]\n",
        "             )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAsGPFdtOH2g"
      },
      "source": [
        "def get_model2(NET):\n",
        " \n",
        " \n",
        "        model = CovidNet()\n",
        "        model.build(input_shape=(None,512, 512, 3))\n",
        "\n",
        "        #opt = CosineDecayRAdam(learning_rate=CFG.LEARNING_RATE, total_steps=int(STEPS_PER_EPOCH*CFG.EPOCHS), warmup_proportion=0.1, min_lr=2e-6)\n",
        "        \n",
        "        #opt = tfa.optimizers.Lookahead(opt)\n",
        "        #model.compile(loss = {'clss': tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
        "        #                      'segg': tf.keras.losses.BinaryCrossentropy(from_logits=True)},\n",
        "        #              metrics = {'clss': [tf.keras.metrics.AUC(multi_label=True),]},\n",
        "        #              optimizer = opt)\n",
        "        #opt =tf.keras.optimizers.Adam(learning_rate=CFG.LEARNING_RATE) \n",
        "        \n",
        "        return model"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCrKyVWxOJDG"
      },
      "source": [
        "k = get_model2(7)"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzGNdx4hOUmt",
        "outputId": "96ac93f7-b7b5-4900-c9f7-62ff85484b9a"
      },
      "source": [
        "k.summary()"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"covid_net_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "model_1 (Functional)         [(None, 16, 16, 2560), (N 64097687  \n",
            "_________________________________________________________________\n",
            "sequential_2 (Sequential)    (None, 4)                 20484     \n",
            "_________________________________________________________________\n",
            "sequential_3 (Sequential)    (None, 32, 32, 1)         406913    \n",
            "=================================================================\n",
            "Total params: 64,525,084\n",
            "Trainable params: 64,208,725\n",
            "Non-trainable params: 316,359\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcTDV0TmOW4p",
        "outputId": "f90d3710-ceb1-4ed6-bf3a-802902985228"
      },
      "source": [
        "for x in train_dataset:\n",
        "    print(x)\n",
        "    break"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(<tf.Tensor: shape=(64, 512, 512, 3), dtype=float32, numpy=\n",
            "array([[[[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
            "\n",
            "        [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
            "\n",
            "        [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
            "\n",
            "        [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
            "\n",
            "        [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]],\n",
            "\n",
            "\n",
            "       [[[8.67156863e-01, 8.67156863e-01, 8.67156863e-01],\n",
            "         [8.76593113e-01, 8.76593113e-01, 8.76593113e-01],\n",
            "         [8.69546592e-01, 8.69546592e-01, 8.69546592e-01],\n",
            "         ...,\n",
            "         [7.08700955e-01, 7.08700955e-01, 7.08700955e-01],\n",
            "         [6.91544116e-01, 6.91544116e-01, 6.91544116e-01],\n",
            "         [6.81004882e-01, 6.81004882e-01, 6.81004882e-01]],\n",
            "\n",
            "        [[8.57107818e-01, 8.57107818e-01, 8.57107818e-01],\n",
            "         [8.60477924e-01, 8.60477924e-01, 8.60477924e-01],\n",
            "         [8.59252453e-01, 8.59252453e-01, 8.59252453e-01],\n",
            "         ...,\n",
            "         [4.02757347e-01, 4.02757347e-01, 4.02757347e-01],\n",
            "         [3.74571085e-01, 3.74571085e-01, 3.74571085e-01],\n",
            "         [3.57659310e-01, 3.57659310e-01, 3.57659310e-01]],\n",
            "\n",
            "        [[8.51899505e-01, 8.51899505e-01, 8.51899505e-01],\n",
            "         [8.51286769e-01, 8.51286769e-01, 8.51286769e-01],\n",
            "         [8.48529398e-01, 8.48529398e-01, 8.48529398e-01],\n",
            "         ...,\n",
            "         [4.83946085e-01, 4.83946085e-01, 4.83946085e-01],\n",
            "         [3.43933821e-01, 3.43933821e-01, 3.43933821e-01],\n",
            "         [1.93749994e-01, 1.93749994e-01, 1.93749994e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[3.15992653e-01, 3.15992653e-01, 3.15992653e-01],\n",
            "         [4.00306374e-01, 4.00306374e-01, 4.00306374e-01],\n",
            "         [3.17279398e-01, 3.17279398e-01, 3.17279398e-01],\n",
            "         ...,\n",
            "         [2.74509806e-02, 2.74509806e-02, 2.74509806e-02],\n",
            "         [2.74509806e-02, 2.74509806e-02, 2.74509806e-02],\n",
            "         [2.74509806e-02, 2.74509806e-02, 2.74509806e-02]],\n",
            "\n",
            "        [[1.51470587e-01, 1.51470587e-01, 1.51470587e-01],\n",
            "         [2.84803927e-01, 2.84803927e-01, 2.84803927e-01],\n",
            "         [2.41605386e-01, 2.41605386e-01, 2.41605386e-01],\n",
            "         ...,\n",
            "         [2.74509806e-02, 2.74509806e-02, 2.74509806e-02],\n",
            "         [2.89828423e-02, 2.89828423e-02, 2.89828423e-02],\n",
            "         [2.99019609e-02, 2.99019609e-02, 2.99019609e-02]],\n",
            "\n",
            "        [[3.39460783e-02, 3.39460783e-02, 3.39460783e-02],\n",
            "         [3.67034301e-02, 3.67034301e-02, 3.67034301e-02],\n",
            "         [4.06862758e-02, 4.06862758e-02, 4.06862758e-02],\n",
            "         ...,\n",
            "         [3.08823530e-02, 3.08823530e-02, 3.08823530e-02],\n",
            "         [3.11887246e-02, 3.11887246e-02, 3.11887246e-02],\n",
            "         [3.13725509e-02, 3.13725509e-02, 3.13725509e-02]]],\n",
            "\n",
            "\n",
            "       [[[1.22120097e-01, 1.22120097e-01, 1.22120097e-01],\n",
            "         [1.19117647e-01, 1.19117647e-01, 1.19117647e-01],\n",
            "         [1.19117647e-01, 1.19117647e-01, 1.19117647e-01],\n",
            "         ...,\n",
            "         [1.47365198e-01, 1.47365198e-01, 1.47365198e-01],\n",
            "         [1.36948526e-01, 1.36948526e-01, 1.36948526e-01],\n",
            "         [9.85294133e-02, 9.85294133e-02, 9.85294133e-02]],\n",
            "\n",
            "        [[1.33823529e-01, 1.33823529e-01, 1.33823529e-01],\n",
            "         [1.33823529e-01, 1.33823529e-01, 1.33823529e-01],\n",
            "         [1.32904410e-01, 1.32904410e-01, 1.32904410e-01],\n",
            "         ...,\n",
            "         [1.53860301e-01, 1.53860301e-01, 1.53860301e-01],\n",
            "         [1.38235301e-01, 1.38235301e-01, 1.38235301e-01],\n",
            "         [9.72426459e-02, 9.72426459e-02, 9.72426459e-02]],\n",
            "\n",
            "        [[1.41176477e-01, 1.41176477e-01, 1.41176477e-01],\n",
            "         [1.41176477e-01, 1.41176477e-01, 1.41176477e-01],\n",
            "         [1.38725489e-01, 1.38725489e-01, 1.38725489e-01],\n",
            "         ...,\n",
            "         [1.55392155e-01, 1.55392155e-01, 1.55392155e-01],\n",
            "         [1.39767155e-01, 1.39767155e-01, 1.39767155e-01],\n",
            "         [9.54044089e-02, 9.54044089e-02, 9.54044089e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[3.59803915e-01, 3.59803915e-01, 3.59803915e-01],\n",
            "         [3.71568620e-01, 3.71568620e-01, 3.71568620e-01],\n",
            "         [3.73100489e-01, 3.73100489e-01, 3.73100489e-01],\n",
            "         ...,\n",
            "         [4.67647046e-01, 4.67647046e-01, 4.67647046e-01],\n",
            "         [4.67647046e-01, 4.67647046e-01, 4.67647046e-01],\n",
            "         [4.53125000e-01, 4.53125000e-01, 4.53125000e-01]],\n",
            "\n",
            "        [[3.59497547e-01, 3.59497547e-01, 3.59497547e-01],\n",
            "         [3.70036751e-01, 3.70036751e-01, 3.70036751e-01],\n",
            "         [3.70036751e-01, 3.70036751e-01, 3.70036751e-01],\n",
            "         ...,\n",
            "         [4.47120100e-01, 4.47120100e-01, 4.47120100e-01],\n",
            "         [4.47120100e-01, 4.47120100e-01, 4.47120100e-01],\n",
            "         [4.31372553e-01, 4.31372553e-01, 4.31372553e-01]],\n",
            "\n",
            "        [[3.44546556e-01, 3.44546556e-01, 3.44546556e-01],\n",
            "         [3.56066167e-01, 3.56066167e-01, 3.56066167e-01],\n",
            "         [3.56066167e-01, 3.56066167e-01, 3.56066167e-01],\n",
            "         ...,\n",
            "         [4.29105401e-01, 4.29105401e-01, 4.29105401e-01],\n",
            "         [4.29105401e-01, 4.29105401e-01, 4.29105401e-01],\n",
            "         [4.12193626e-01, 4.12193626e-01, 4.12193626e-01]]],\n",
            "\n",
            "\n",
            "       ...,\n",
            "\n",
            "\n",
            "       [[[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
            "\n",
            "        [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
            "\n",
            "        [[6.12745120e-04, 6.12745120e-04, 6.12745120e-04],\n",
            "         [3.06372554e-03, 3.06372554e-03, 3.06372554e-03],\n",
            "         [1.53186277e-03, 1.53186277e-03, 1.53186277e-03],\n",
            "         ...,\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
            "\n",
            "        [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
            "\n",
            "        [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]],\n",
            "\n",
            "\n",
            "       [[[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [5.18566191e-01, 5.18566191e-01, 5.18566191e-01],\n",
            "         [7.04227924e-01, 7.04227924e-01, 7.04227924e-01],\n",
            "         [6.70343161e-01, 6.70343161e-01, 6.70343161e-01]],\n",
            "\n",
            "        [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [5.05882382e-01, 5.05882382e-01, 5.05882382e-01],\n",
            "         [6.86335802e-01, 6.86335802e-01, 6.86335802e-01],\n",
            "         [6.65196061e-01, 6.65196061e-01, 6.65196061e-01]],\n",
            "\n",
            "        [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [5.07720590e-01, 5.07720590e-01, 5.07720590e-01],\n",
            "         [6.88174009e-01, 6.88174009e-01, 6.88174009e-01],\n",
            "         [6.56617641e-01, 6.56617641e-01, 6.56617641e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[1.01960786e-01, 1.01960786e-01, 1.01960786e-01],\n",
            "         [6.56862780e-02, 6.56862780e-02, 6.56862780e-02],\n",
            "         [4.60784324e-02, 4.60784324e-02, 4.60784324e-02],\n",
            "         ...,\n",
            "         [9.11764726e-02, 9.11764726e-02, 9.11764726e-02],\n",
            "         [1.23039216e-01, 1.23039216e-01, 1.23039216e-01],\n",
            "         [8.33333358e-02, 8.33333358e-02, 8.33333358e-02]],\n",
            "\n",
            "        [[1.04411766e-01, 1.04411766e-01, 1.04411766e-01],\n",
            "         [6.72181398e-02, 6.72181398e-02, 6.72181398e-02],\n",
            "         [4.60784324e-02, 4.60784324e-02, 4.60784324e-02],\n",
            "         ...,\n",
            "         [9.69975516e-02, 9.69975516e-02, 9.69975516e-02],\n",
            "         [1.28860295e-01, 1.28860295e-01, 1.28860295e-01],\n",
            "         [9.03799012e-02, 9.03799012e-02, 9.03799012e-02]],\n",
            "\n",
            "        [[1.08884804e-01, 1.08884804e-01, 1.08884804e-01],\n",
            "         [6.81372583e-02, 6.81372583e-02, 6.81372583e-02],\n",
            "         [4.60784324e-02, 4.60784324e-02, 4.60784324e-02],\n",
            "         ...,\n",
            "         [1.23039216e-01, 1.23039216e-01, 1.23039216e-01],\n",
            "         [1.54901966e-01, 1.54901966e-01, 1.54901966e-01],\n",
            "         [1.14154413e-01, 1.14154413e-01, 1.14154413e-01]]],\n",
            "\n",
            "\n",
            "       [[[3.38725477e-01, 3.38725477e-01, 3.38725477e-01],\n",
            "         [2.73529410e-01, 2.73529410e-01, 2.73529410e-01],\n",
            "         [2.14399517e-01, 2.14399517e-01, 2.14399517e-01],\n",
            "         ...,\n",
            "         [2.80085772e-01, 2.80085772e-01, 2.80085772e-01],\n",
            "         [3.90686274e-01, 3.90686274e-01, 3.90686274e-01],\n",
            "         [3.22058827e-01, 3.22058827e-01, 3.22058827e-01]],\n",
            "\n",
            "        [[3.38541657e-01, 3.38541657e-01, 3.38541657e-01],\n",
            "         [2.71507353e-01, 2.71507353e-01, 2.71507353e-01],\n",
            "         [2.10232839e-01, 2.10232839e-01, 2.10232839e-01],\n",
            "         ...,\n",
            "         [2.79901952e-01, 2.79901952e-01, 2.79901952e-01],\n",
            "         [3.91727954e-01, 3.91727954e-01, 3.91727954e-01],\n",
            "         [3.22549015e-01, 3.22549015e-01, 3.22549015e-01]],\n",
            "\n",
            "        [[3.33333343e-01, 3.33333343e-01, 3.33333343e-01],\n",
            "         [2.64154404e-01, 2.64154404e-01, 2.64154404e-01],\n",
            "         [2.04411760e-01, 2.04411760e-01, 2.04411760e-01],\n",
            "         ...,\n",
            "         [2.76531875e-01, 2.76531875e-01, 2.76531875e-01],\n",
            "         [3.85294110e-01, 3.85294110e-01, 3.85294110e-01],\n",
            "         [3.15196067e-01, 3.15196067e-01, 3.15196067e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[3.54901969e-01, 3.54901969e-01, 3.54901969e-01],\n",
            "         [3.19117635e-01, 3.19117635e-01, 3.19117635e-01],\n",
            "         [2.81433821e-01, 2.81433821e-01, 2.81433821e-01],\n",
            "         ...,\n",
            "         [7.76041687e-01, 7.76041687e-01, 7.76041687e-01],\n",
            "         [8.21997523e-01, 8.21997523e-01, 8.21997523e-01],\n",
            "         [5.93504906e-01, 5.93504906e-01, 5.93504906e-01]],\n",
            "\n",
            "        [[4.06678915e-01, 4.06678915e-01, 4.06678915e-01],\n",
            "         [3.73039216e-01, 3.73039216e-01, 3.73039216e-01],\n",
            "         [3.33823532e-01, 3.33823532e-01, 3.33823532e-01],\n",
            "         ...,\n",
            "         [8.52022052e-01, 8.52022052e-01, 8.52022052e-01],\n",
            "         [9.19423997e-01, 9.19423997e-01, 9.19423997e-01],\n",
            "         [6.20465696e-01, 6.20465696e-01, 6.20465696e-01]],\n",
            "\n",
            "        [[3.86764705e-01, 3.86764705e-01, 3.86764705e-01],\n",
            "         [3.55698526e-01, 3.55698526e-01, 3.55698526e-01],\n",
            "         [3.20772052e-01, 3.20772052e-01, 3.20772052e-01],\n",
            "         ...,\n",
            "         [7.22916663e-01, 7.22916663e-01, 7.22916663e-01],\n",
            "         [7.75919139e-01, 7.75919139e-01, 7.75919139e-01],\n",
            "         [5.19056380e-01, 5.19056380e-01, 5.19056380e-01]]]],\n",
            "      dtype=float32)>, {'clss': <tf.Tensor: shape=(64, 4), dtype=float32, numpy=\n",
            "array([[0., 0., 0., 1.],\n",
            "       [0., 0., 0., 1.],\n",
            "       [0., 0., 0., 1.],\n",
            "       [0., 0., 1., 0.],\n",
            "       [0., 0., 0., 1.],\n",
            "       [1., 0., 0., 0.],\n",
            "       [0., 0., 0., 1.],\n",
            "       [0., 0., 0., 1.],\n",
            "       [1., 0., 0., 0.],\n",
            "       [0., 0., 0., 1.],\n",
            "       [1., 0., 0., 0.],\n",
            "       [1., 0., 0., 0.],\n",
            "       [1., 0., 0., 0.],\n",
            "       [0., 1., 0., 0.],\n",
            "       [0., 0., 1., 0.],\n",
            "       [0., 1., 0., 0.],\n",
            "       [0., 0., 0., 1.],\n",
            "       [1., 0., 0., 0.],\n",
            "       [0., 0., 0., 1.],\n",
            "       [0., 1., 0., 0.],\n",
            "       [0., 1., 0., 0.],\n",
            "       [1., 0., 0., 0.],\n",
            "       [0., 0., 0., 1.],\n",
            "       [1., 0., 0., 0.],\n",
            "       [0., 0., 0., 1.],\n",
            "       [0., 1., 0., 0.],\n",
            "       [0., 1., 0., 0.],\n",
            "       [0., 0., 0., 1.],\n",
            "       [0., 1., 0., 0.],\n",
            "       [1., 0., 0., 0.],\n",
            "       [1., 0., 0., 0.],\n",
            "       [0., 0., 0., 1.],\n",
            "       [0., 0., 0., 1.],\n",
            "       [0., 0., 0., 1.],\n",
            "       [0., 1., 0., 0.],\n",
            "       [0., 0., 0., 1.],\n",
            "       [0., 1., 0., 0.],\n",
            "       [0., 1., 0., 0.],\n",
            "       [0., 1., 0., 0.],\n",
            "       [0., 0., 0., 1.],\n",
            "       [0., 0., 0., 1.],\n",
            "       [0., 0., 1., 0.],\n",
            "       [0., 0., 0., 1.],\n",
            "       [1., 0., 0., 0.],\n",
            "       [0., 0., 0., 1.],\n",
            "       [0., 1., 0., 0.],\n",
            "       [0., 0., 0., 1.],\n",
            "       [1., 0., 0., 0.],\n",
            "       [1., 0., 0., 0.],\n",
            "       [1., 0., 0., 0.],\n",
            "       [0., 1., 0., 0.],\n",
            "       [1., 0., 0., 0.],\n",
            "       [0., 0., 1., 0.],\n",
            "       [1., 0., 0., 0.],\n",
            "       [0., 0., 0., 1.],\n",
            "       [1., 0., 0., 0.],\n",
            "       [0., 0., 0., 1.],\n",
            "       [0., 0., 0., 1.],\n",
            "       [0., 1., 0., 0.],\n",
            "       [0., 0., 1., 0.],\n",
            "       [0., 0., 0., 1.],\n",
            "       [0., 1., 0., 0.],\n",
            "       [1., 0., 0., 0.],\n",
            "       [0., 1., 0., 0.]], dtype=float32)>, 'segg': <tf.Tensor: shape=(64, 32, 32, 1), dtype=float32, numpy=\n",
            "array([[[[0.        ],\n",
            "         [0.        ],\n",
            "         [0.        ],\n",
            "         ...,\n",
            "         [0.        ],\n",
            "         [0.        ],\n",
            "         [0.        ]],\n",
            "\n",
            "        [[0.41568628],\n",
            "         [0.39215687],\n",
            "         [0.38039216],\n",
            "         ...,\n",
            "         [0.34117648],\n",
            "         [0.36078432],\n",
            "         [0.38039216]],\n",
            "\n",
            "        [[0.4627451 ],\n",
            "         [0.41568628],\n",
            "         [0.4117647 ],\n",
            "         ...,\n",
            "         [0.50980395],\n",
            "         [0.29411766],\n",
            "         [0.34901962]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.5647059 ],\n",
            "         [0.54901963],\n",
            "         [0.24705882],\n",
            "         ...,\n",
            "         [0.63529414],\n",
            "         [0.18039216],\n",
            "         [0.36078432]],\n",
            "\n",
            "        [[0.53333336],\n",
            "         [0.23137255],\n",
            "         [0.30980393],\n",
            "         ...,\n",
            "         [0.3647059 ],\n",
            "         [0.5137255 ],\n",
            "         [0.36078432]],\n",
            "\n",
            "        [[0.        ],\n",
            "         [0.        ],\n",
            "         [0.        ],\n",
            "         ...,\n",
            "         [0.        ],\n",
            "         [0.        ],\n",
            "         [0.        ]]],\n",
            "\n",
            "\n",
            "       [[[0.85882354],\n",
            "         [0.9137255 ],\n",
            "         [0.7921569 ],\n",
            "         ...,\n",
            "         [0.02745098],\n",
            "         [0.01176471],\n",
            "         [0.00392157]],\n",
            "\n",
            "        [[0.48235294],\n",
            "         [0.23921569],\n",
            "         [0.1764706 ],\n",
            "         ...,\n",
            "         [0.01176471],\n",
            "         [0.00784314],\n",
            "         [0.05098039]],\n",
            "\n",
            "        [[0.01176471],\n",
            "         [0.00784314],\n",
            "         [0.00392157],\n",
            "         ...,\n",
            "         [0.02352941],\n",
            "         [0.01960784],\n",
            "         [0.03137255]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.00392157],\n",
            "         [0.02745098],\n",
            "         [0.        ],\n",
            "         ...,\n",
            "         [0.01176471],\n",
            "         [0.00392157],\n",
            "         [0.        ]],\n",
            "\n",
            "        [[0.00784314],\n",
            "         [0.        ],\n",
            "         [0.01960784],\n",
            "         ...,\n",
            "         [0.00392157],\n",
            "         [0.01960784],\n",
            "         [0.        ]],\n",
            "\n",
            "        [[0.        ],\n",
            "         [0.01568628],\n",
            "         [0.        ],\n",
            "         ...,\n",
            "         [0.00784314],\n",
            "         [0.00784314],\n",
            "         [0.        ]]],\n",
            "\n",
            "\n",
            "       [[[0.10588235],\n",
            "         [0.29803923],\n",
            "         [0.3372549 ],\n",
            "         ...,\n",
            "         [0.09019608],\n",
            "         [0.10196079],\n",
            "         [0.13333334]],\n",
            "\n",
            "        [[0.5254902 ],\n",
            "         [0.53333336],\n",
            "         [0.5058824 ],\n",
            "         ...,\n",
            "         [0.33333334],\n",
            "         [0.25882354],\n",
            "         [0.09411765]],\n",
            "\n",
            "        [[0.5411765 ],\n",
            "         [0.53333336],\n",
            "         [0.5058824 ],\n",
            "         ...,\n",
            "         [0.52156866],\n",
            "         [0.36862746],\n",
            "         [0.27450982]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.35686275],\n",
            "         [0.4509804 ],\n",
            "         [0.5372549 ],\n",
            "         ...,\n",
            "         [0.6       ],\n",
            "         [0.47058824],\n",
            "         [0.4509804 ]],\n",
            "\n",
            "        [[0.36862746],\n",
            "         [0.3372549 ],\n",
            "         [0.5176471 ],\n",
            "         ...,\n",
            "         [0.5647059 ],\n",
            "         [0.43137255],\n",
            "         [0.44313726]],\n",
            "\n",
            "        [[0.37254903],\n",
            "         [0.39215687],\n",
            "         [0.5294118 ],\n",
            "         ...,\n",
            "         [0.48235294],\n",
            "         [0.35686275],\n",
            "         [0.40392157]]],\n",
            "\n",
            "\n",
            "       ...,\n",
            "\n",
            "\n",
            "       [[[0.01960784],\n",
            "         [0.11372549],\n",
            "         [0.        ],\n",
            "         ...,\n",
            "         [0.        ],\n",
            "         [0.        ],\n",
            "         [0.        ]],\n",
            "\n",
            "        [[0.        ],\n",
            "         [0.        ],\n",
            "         [0.        ],\n",
            "         ...,\n",
            "         [0.        ],\n",
            "         [0.        ],\n",
            "         [0.        ]],\n",
            "\n",
            "        [[0.        ],\n",
            "         [0.        ],\n",
            "         [0.        ],\n",
            "         ...,\n",
            "         [0.04313726],\n",
            "         [0.01568628],\n",
            "         [0.        ]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.        ],\n",
            "         [0.        ],\n",
            "         [0.48235294],\n",
            "         ...,\n",
            "         [0.88235295],\n",
            "         [0.78039217],\n",
            "         [0.        ]],\n",
            "\n",
            "        [[0.        ],\n",
            "         [0.        ],\n",
            "         [0.        ],\n",
            "         ...,\n",
            "         [0.        ],\n",
            "         [0.        ],\n",
            "         [0.        ]],\n",
            "\n",
            "        [[0.        ],\n",
            "         [0.        ],\n",
            "         [0.        ],\n",
            "         ...,\n",
            "         [0.        ],\n",
            "         [0.        ],\n",
            "         [0.        ]]],\n",
            "\n",
            "\n",
            "       [[[0.        ],\n",
            "         [0.        ],\n",
            "         [0.        ],\n",
            "         ...,\n",
            "         [0.03529412],\n",
            "         [0.00392157],\n",
            "         [0.02352941]],\n",
            "\n",
            "        [[0.        ],\n",
            "         [0.        ],\n",
            "         [0.00784314],\n",
            "         ...,\n",
            "         [0.04313726],\n",
            "         [0.00392157],\n",
            "         [0.        ]],\n",
            "\n",
            "        [[0.02745098],\n",
            "         [0.02352941],\n",
            "         [0.        ],\n",
            "         ...,\n",
            "         [0.2509804 ],\n",
            "         [0.19215687],\n",
            "         [0.12156863]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.01568628],\n",
            "         [0.01568628],\n",
            "         [0.03921569],\n",
            "         ...,\n",
            "         [0.02352941],\n",
            "         [0.02745098],\n",
            "         [0.02745098]],\n",
            "\n",
            "        [[0.00784314],\n",
            "         [0.02352941],\n",
            "         [0.02745098],\n",
            "         ...,\n",
            "         [0.01568628],\n",
            "         [0.03921569],\n",
            "         [0.02352941]],\n",
            "\n",
            "        [[0.01176471],\n",
            "         [0.01176471],\n",
            "         [0.03529412],\n",
            "         ...,\n",
            "         [0.00784314],\n",
            "         [0.03137255],\n",
            "         [0.02352941]]],\n",
            "\n",
            "\n",
            "       [[[0.01568628],\n",
            "         [0.        ],\n",
            "         [0.        ],\n",
            "         ...,\n",
            "         [0.04705882],\n",
            "         [0.        ],\n",
            "         [0.05098039]],\n",
            "\n",
            "        [[0.02352941],\n",
            "         [0.        ],\n",
            "         [0.        ],\n",
            "         ...,\n",
            "         [0.47843137],\n",
            "         [0.4627451 ],\n",
            "         [0.35686275]],\n",
            "\n",
            "        [[0.01568628],\n",
            "         [0.        ],\n",
            "         [0.15294118],\n",
            "         ...,\n",
            "         [0.58431375],\n",
            "         [0.6117647 ],\n",
            "         [0.5882353 ]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.        ],\n",
            "         [0.        ],\n",
            "         [0.        ],\n",
            "         ...,\n",
            "         [0.6509804 ],\n",
            "         [0.6156863 ],\n",
            "         [0.5686275 ]],\n",
            "\n",
            "        [[0.        ],\n",
            "         [0.        ],\n",
            "         [0.        ],\n",
            "         ...,\n",
            "         [0.5647059 ],\n",
            "         [0.5921569 ],\n",
            "         [0.5529412 ]],\n",
            "\n",
            "        [[0.        ],\n",
            "         [0.        ],\n",
            "         [0.01176471],\n",
            "         ...,\n",
            "         [0.72156864],\n",
            "         [0.6       ],\n",
            "         [0.5568628 ]]]], dtype=float32)>})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqSesHR1PtQU",
        "outputId": "461835fd-7ba3-4b56-d5a6-ebaf4be02740"
      },
      "source": [
        "x[0].shape"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 512, 512, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_AqYSPdOe_i",
        "outputId": "db9c250a-71a5-4a73-b405-347831aeb755"
      },
      "source": [
        "x[1]['clss'].shape"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbRwmF_RRSCa",
        "outputId": "6a0b28f5-2962-43f1-87e1-f341e625b140"
      },
      "source": [
        "k.summary()"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"covid_net_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "model_1 (Functional)         [(None, 16, 16, 2560), (N 64097687  \n",
            "_________________________________________________________________\n",
            "sequential_2 (Sequential)    (None, 4)                 20484     \n",
            "_________________________________________________________________\n",
            "sequential_3 (Sequential)    (None, 32, 32, 1)         406913    \n",
            "=================================================================\n",
            "Total params: 64,525,084\n",
            "Trainable params: 64,208,725\n",
            "Non-trainable params: 316,359\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQiAcn6COxvH",
        "outputId": "09c4b1b0-6582-4136-fdb8-0ce7c508d5ef"
      },
      "source": [
        "k(tf.random.uniform(shape=(64,512,512,3)))"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'clss': <tf.Tensor: shape=(64, 4), dtype=float32, numpy=\n",
              " array([[0.24851857, 0.24563384, 0.25108683, 0.25476083],\n",
              "        [0.24856198, 0.24565056, 0.2510477 , 0.25473976],\n",
              "        [0.24852742, 0.24566807, 0.25105974, 0.25474474],\n",
              "        [0.24856582, 0.24562666, 0.2510524 , 0.25475514],\n",
              "        [0.24855427, 0.2456628 , 0.25103948, 0.25474352],\n",
              "        [0.24855901, 0.24565803, 0.25105053, 0.25473243],\n",
              "        [0.24855602, 0.24564606, 0.2510502 , 0.25474778],\n",
              "        [0.24852663, 0.24565   , 0.2510695 , 0.25475392],\n",
              "        [0.24854061, 0.2456473 , 0.25105134, 0.2547607 ],\n",
              "        [0.24852666, 0.2456552 , 0.25106904, 0.25474915],\n",
              "        [0.24858122, 0.24565373, 0.25102285, 0.25474223],\n",
              "        [0.24854575, 0.24564946, 0.25105533, 0.25474948],\n",
              "        [0.24855712, 0.24563971, 0.25104645, 0.25475675],\n",
              "        [0.24856389, 0.24563123, 0.25104943, 0.2547555 ],\n",
              "        [0.24855852, 0.24563244, 0.25105056, 0.25475848],\n",
              "        [0.24857847, 0.24563679, 0.25103173, 0.25475305],\n",
              "        [0.24851781, 0.24565268, 0.25108534, 0.2547442 ],\n",
              "        [0.2485572 , 0.24562235, 0.25105754, 0.25476286],\n",
              "        [0.24854913, 0.245649  , 0.25105128, 0.25475052],\n",
              "        [0.24855733, 0.24565957, 0.25104052, 0.25474253],\n",
              "        [0.24854182, 0.24565402, 0.25105324, 0.254751  ],\n",
              "        [0.24853216, 0.24562834, 0.2510718 , 0.25476766],\n",
              "        [0.24855004, 0.24563913, 0.25105435, 0.25475642],\n",
              "        [0.2485464 , 0.24563988, 0.2510552 , 0.2547585 ],\n",
              "        [0.24856617, 0.24561623, 0.25104794, 0.25476968],\n",
              "        [0.24852856, 0.24562575, 0.25107932, 0.2547664 ],\n",
              "        [0.24857065, 0.2456405 , 0.2510405 , 0.2547484 ],\n",
              "        [0.24858855, 0.24562506, 0.25102693, 0.25475937],\n",
              "        [0.24857745, 0.24564019, 0.25102922, 0.2547532 ],\n",
              "        [0.2485525 , 0.24564323, 0.2510493 , 0.25475505],\n",
              "        [0.2484864 , 0.24565418, 0.25109088, 0.25476852],\n",
              "        [0.24855877, 0.2456241 , 0.25105372, 0.25476333],\n",
              "        [0.24852741, 0.24565294, 0.25107303, 0.25474668],\n",
              "        [0.24858433, 0.24563539, 0.25103042, 0.2547499 ],\n",
              "        [0.24850908, 0.24562025, 0.25109223, 0.25477841],\n",
              "        [0.24856262, 0.24563716, 0.25104088, 0.25475925],\n",
              "        [0.24857855, 0.24564618, 0.25103608, 0.2547392 ],\n",
              "        [0.24856374, 0.24563636, 0.25105318, 0.25474676],\n",
              "        [0.24857298, 0.24563983, 0.2510365 , 0.25475076],\n",
              "        [0.24850304, 0.24564014, 0.25109506, 0.25476173],\n",
              "        [0.24856825, 0.24565504, 0.25103977, 0.25473693],\n",
              "        [0.24857314, 0.24563457, 0.25104102, 0.25475124],\n",
              "        [0.24854705, 0.24564873, 0.25105557, 0.2547487 ],\n",
              "        [0.24854891, 0.24565372, 0.25105345, 0.25474387],\n",
              "        [0.24852476, 0.24563755, 0.25107768, 0.25476   ],\n",
              "        [0.24852367, 0.24563049, 0.2510746 , 0.25477126],\n",
              "        [0.24854751, 0.24563   , 0.25106618, 0.25475633],\n",
              "        [0.24857105, 0.2456539 , 0.25103343, 0.25474155],\n",
              "        [0.24855219, 0.24565306, 0.251046  , 0.25474876],\n",
              "        [0.24854071, 0.24565405, 0.25106102, 0.2547442 ],\n",
              "        [0.24853064, 0.2456375 , 0.25106776, 0.2547641 ],\n",
              "        [0.24855076, 0.2456585 , 0.25104868, 0.25474206],\n",
              "        [0.24850287, 0.24562892, 0.2510898 , 0.2547784 ],\n",
              "        [0.24858156, 0.24561797, 0.25104687, 0.25475356],\n",
              "        [0.24854358, 0.24562773, 0.25107196, 0.2547567 ],\n",
              "        [0.24851823, 0.2456413 , 0.25107747, 0.25476307],\n",
              "        [0.24854372, 0.24565221, 0.25104564, 0.25475845],\n",
              "        [0.2485221 , 0.24563448, 0.25108558, 0.25475785],\n",
              "        [0.24854146, 0.24565548, 0.2510459 , 0.25475717],\n",
              "        [0.2485502 , 0.24562545, 0.25106362, 0.25476074],\n",
              "        [0.24852516, 0.2456361 , 0.25108087, 0.2547579 ],\n",
              "        [0.24856353, 0.24564049, 0.2510377 , 0.25475824],\n",
              "        [0.24851058, 0.24564807, 0.2510843 , 0.25475708],\n",
              "        [0.24854131, 0.2456326 , 0.25106123, 0.25476485]], dtype=float32)>,\n",
              " 'segg': <tf.Tensor: shape=(64, 32, 32, 1), dtype=float32, numpy=\n",
              " array([[[[ 3.0921149e+00],\n",
              "          [-6.7489219e-01],\n",
              "          [ 7.4684000e-01],\n",
              "          ...,\n",
              "          [ 1.4565473e+00],\n",
              "          [ 2.2954612e+00],\n",
              "          [-9.0405369e-01]],\n",
              " \n",
              "         [[ 3.0370836e+00],\n",
              "          [-2.3799403e+00],\n",
              "          [-1.4662120e+00],\n",
              "          ...,\n",
              "          [ 1.1021109e+00],\n",
              "          [ 1.1674426e+00],\n",
              "          [-3.1135228e+00]],\n",
              " \n",
              "         [[ 1.7353776e+00],\n",
              "          [-1.5667075e+00],\n",
              "          [-3.0417395e-01],\n",
              "          ...,\n",
              "          [ 6.3161850e-03],\n",
              "          [ 1.9020422e+00],\n",
              "          [-3.2742667e-01]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[-1.9777334e-01],\n",
              "          [-5.6263185e+00],\n",
              "          [-2.7371655e+00],\n",
              "          ...,\n",
              "          [-2.5063453e+00],\n",
              "          [-4.3234801e-01],\n",
              "          [-8.0933809e-01]],\n",
              " \n",
              "         [[ 1.4334450e+00],\n",
              "          [-4.6020517e+00],\n",
              "          [-1.2424119e+00],\n",
              "          ...,\n",
              "          [-1.4659424e+00],\n",
              "          [ 1.0574195e+00],\n",
              "          [ 7.1087718e-01]],\n",
              " \n",
              "         [[-1.9399503e+00],\n",
              "          [-7.1792135e+00],\n",
              "          [-4.0198345e+00],\n",
              "          ...,\n",
              "          [-5.3308473e+00],\n",
              "          [-3.5076919e+00],\n",
              "          [-5.1578569e+00]]],\n",
              " \n",
              " \n",
              "        [[[ 3.0482969e+00],\n",
              "          [-6.9394016e-01],\n",
              "          [ 7.4419093e-01],\n",
              "          ...,\n",
              "          [ 1.4873278e+00],\n",
              "          [ 2.3144946e+00],\n",
              "          [-9.1988611e-01]],\n",
              " \n",
              "         [[ 3.0708976e+00],\n",
              "          [-2.4027486e+00],\n",
              "          [-1.4601393e+00],\n",
              "          ...,\n",
              "          [ 1.1277342e+00],\n",
              "          [ 1.1677580e+00],\n",
              "          [-3.0964282e+00]],\n",
              " \n",
              "         [[ 1.6798830e+00],\n",
              "          [-1.5753539e+00],\n",
              "          [-2.7561665e-01],\n",
              "          ...,\n",
              "          [ 1.4385700e-02],\n",
              "          [ 1.9171941e+00],\n",
              "          [-3.1664228e-01]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[-1.7337418e-01],\n",
              "          [-5.5933199e+00],\n",
              "          [-2.7486427e+00],\n",
              "          ...,\n",
              "          [-2.4974198e+00],\n",
              "          [-4.3895411e-01],\n",
              "          [-8.0756044e-01]],\n",
              " \n",
              "         [[ 1.4296169e+00],\n",
              "          [-4.5613899e+00],\n",
              "          [-1.2322640e+00],\n",
              "          ...,\n",
              "          [-1.4668673e+00],\n",
              "          [ 1.0549088e+00],\n",
              "          [ 7.4423122e-01]],\n",
              " \n",
              "         [[-1.9514883e+00],\n",
              "          [-7.1435161e+00],\n",
              "          [-4.0412445e+00],\n",
              "          ...,\n",
              "          [-5.3386388e+00],\n",
              "          [-3.4907491e+00],\n",
              "          [-5.1761584e+00]]],\n",
              " \n",
              " \n",
              "        [[[ 3.0712709e+00],\n",
              "          [-6.9098878e-01],\n",
              "          [ 7.4326539e-01],\n",
              "          ...,\n",
              "          [ 1.4458652e+00],\n",
              "          [ 2.2838092e+00],\n",
              "          [-9.1103888e-01]],\n",
              " \n",
              "         [[ 3.0691652e+00],\n",
              "          [-2.3855877e+00],\n",
              "          [-1.4660914e+00],\n",
              "          ...,\n",
              "          [ 1.1031818e+00],\n",
              "          [ 1.1474462e+00],\n",
              "          [-3.1050186e+00]],\n",
              " \n",
              "         [[ 1.6912427e+00],\n",
              "          [-1.5863812e+00],\n",
              "          [-2.9439735e-01],\n",
              "          ...,\n",
              "          [ 7.5898170e-03],\n",
              "          [ 1.9013331e+00],\n",
              "          [-3.1699920e-01]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[-1.8528730e-01],\n",
              "          [-5.6208754e+00],\n",
              "          [-2.7351632e+00],\n",
              "          ...,\n",
              "          [-2.5004306e+00],\n",
              "          [-4.3840456e-01],\n",
              "          [-8.1162095e-01]],\n",
              " \n",
              "         [[ 1.4211094e+00],\n",
              "          [-4.5987358e+00],\n",
              "          [-1.2347977e+00],\n",
              "          ...,\n",
              "          [-1.4747251e+00],\n",
              "          [ 1.0498776e+00],\n",
              "          [ 7.4724770e-01]],\n",
              " \n",
              "         [[-1.9676118e+00],\n",
              "          [-7.1887169e+00],\n",
              "          [-4.0488300e+00],\n",
              "          ...,\n",
              "          [-5.3397713e+00],\n",
              "          [-3.4949851e+00],\n",
              "          [-5.1962929e+00]]],\n",
              " \n",
              " \n",
              "        ...,\n",
              " \n",
              " \n",
              "        [[[ 3.0619779e+00],\n",
              "          [-6.6044331e-01],\n",
              "          [ 7.5073028e-01],\n",
              "          ...,\n",
              "          [ 1.4754488e+00],\n",
              "          [ 2.3062830e+00],\n",
              "          [-9.3894148e-01]],\n",
              " \n",
              "         [[ 3.0245154e+00],\n",
              "          [-2.3528285e+00],\n",
              "          [-1.4737887e+00],\n",
              "          ...,\n",
              "          [ 1.1125772e+00],\n",
              "          [ 1.1557775e+00],\n",
              "          [-3.0891516e+00]],\n",
              " \n",
              "         [[ 1.7090369e+00],\n",
              "          [-1.5630994e+00],\n",
              "          [-3.1103158e-01],\n",
              "          ...,\n",
              "          [ 2.9105425e-02],\n",
              "          [ 1.9295082e+00],\n",
              "          [-3.0396867e-01]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[-1.9061065e-01],\n",
              "          [-5.6298709e+00],\n",
              "          [-2.7637823e+00],\n",
              "          ...,\n",
              "          [-2.5111880e+00],\n",
              "          [-4.3819976e-01],\n",
              "          [-8.0014491e-01]],\n",
              " \n",
              "         [[ 1.4115853e+00],\n",
              "          [-4.6049700e+00],\n",
              "          [-1.2516661e+00],\n",
              "          ...,\n",
              "          [-1.4498519e+00],\n",
              "          [ 1.0719743e+00],\n",
              "          [ 7.2442627e-01]],\n",
              " \n",
              "         [[-1.9428170e+00],\n",
              "          [-7.1910543e+00],\n",
              "          [-4.0444117e+00],\n",
              "          ...,\n",
              "          [-5.2974501e+00],\n",
              "          [-3.4893379e+00],\n",
              "          [-5.1677470e+00]]],\n",
              " \n",
              " \n",
              "        [[[ 3.0605016e+00],\n",
              "          [-6.7481661e-01],\n",
              "          [ 7.5063348e-01],\n",
              "          ...,\n",
              "          [ 1.4654007e+00],\n",
              "          [ 2.2881002e+00],\n",
              "          [-9.1210985e-01]],\n",
              " \n",
              "         [[ 3.0360086e+00],\n",
              "          [-2.4099011e+00],\n",
              "          [-1.4770522e+00],\n",
              "          ...,\n",
              "          [ 1.1076710e+00],\n",
              "          [ 1.1602001e+00],\n",
              "          [-3.1173303e+00]],\n",
              " \n",
              "         [[ 1.7122555e+00],\n",
              "          [-1.6230897e+00],\n",
              "          [-2.6463175e-01],\n",
              "          ...,\n",
              "          [ 1.5666246e-02],\n",
              "          [ 1.8842402e+00],\n",
              "          [-3.2712841e-01]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[-1.8378276e-01],\n",
              "          [-5.6150632e+00],\n",
              "          [-2.7347445e+00],\n",
              "          ...,\n",
              "          [-2.5273414e+00],\n",
              "          [-4.5635295e-01],\n",
              "          [-8.3656335e-01]],\n",
              " \n",
              "         [[ 1.4162114e+00],\n",
              "          [-4.6049576e+00],\n",
              "          [-1.2404375e+00],\n",
              "          ...,\n",
              "          [-1.4880590e+00],\n",
              "          [ 1.0361390e+00],\n",
              "          [ 7.3214388e-01]],\n",
              " \n",
              "         [[-1.9555542e+00],\n",
              "          [-7.1794891e+00],\n",
              "          [-4.0133810e+00],\n",
              "          ...,\n",
              "          [-5.3376546e+00],\n",
              "          [-3.5033855e+00],\n",
              "          [-5.1990066e+00]]],\n",
              " \n",
              " \n",
              "        [[[ 3.0575428e+00],\n",
              "          [-6.8102217e-01],\n",
              "          [ 7.5279427e-01],\n",
              "          ...,\n",
              "          [ 1.4711651e+00],\n",
              "          [ 2.3028867e+00],\n",
              "          [-9.4138241e-01]],\n",
              " \n",
              "         [[ 3.0469258e+00],\n",
              "          [-2.3871918e+00],\n",
              "          [-1.4721174e+00],\n",
              "          ...,\n",
              "          [ 1.1345634e+00],\n",
              "          [ 1.1722693e+00],\n",
              "          [-3.0939143e+00]],\n",
              " \n",
              "         [[ 1.7165222e+00],\n",
              "          [-1.5704527e+00],\n",
              "          [-2.9214525e-01],\n",
              "          ...,\n",
              "          [ 1.4577627e-02],\n",
              "          [ 1.9249480e+00],\n",
              "          [-2.9881144e-01]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[-2.0532304e-01],\n",
              "          [-5.6438651e+00],\n",
              "          [-2.7446256e+00],\n",
              "          ...,\n",
              "          [-2.5350761e+00],\n",
              "          [-4.3652964e-01],\n",
              "          [-8.1933045e-01]],\n",
              " \n",
              "         [[ 1.4290142e+00],\n",
              "          [-4.6154013e+00],\n",
              "          [-1.2465618e+00],\n",
              "          ...,\n",
              "          [-1.4685532e+00],\n",
              "          [ 1.0271094e+00],\n",
              "          [ 7.1262074e-01]],\n",
              " \n",
              "         [[-1.9326472e+00],\n",
              "          [-7.1768155e+00],\n",
              "          [-4.0245581e+00],\n",
              "          ...,\n",
              "          [-5.3301702e+00],\n",
              "          [-3.5361097e+00],\n",
              "          [-5.2043567e+00]]]], dtype=float32)>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWWQKgj8S1Gs"
      },
      "source": [
        "from tensorflow.keras.optimizers.schedules import LearningRateSchedule, ExponentialDecay\n",
        "\n",
        "class WarmupLearningRateSchedule(LearningRateSchedule):\n",
        "    \"\"\"Provides a variety of learning rate decay schedules with warm up.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "               initial_lr,\n",
        "               steps_per_epoch=None,\n",
        "               lr_decay_type='exponential',\n",
        "               decay_factor=0.97,\n",
        "               decay_epochs=2.4,\n",
        "               total_steps=None,\n",
        "               warmup_epochs=5,\n",
        "               minimal_lr=0):\n",
        "        super(WarmupLearningRateSchedule, self).__init__()\n",
        "        self.initial_lr = initial_lr\n",
        "        self.steps_per_epoch = steps_per_epoch\n",
        "        self.lr_decay_type = lr_decay_type\n",
        "        self.decay_factor = decay_factor\n",
        "        self.decay_epochs = decay_epochs\n",
        "        self.total_steps = total_steps\n",
        "        self.warmup_epochs = warmup_epochs\n",
        "        self.minimal_lr = minimal_lr\n",
        "\n",
        "    def __call__(self, step):\n",
        "        if self.lr_decay_type == 'exponential':\n",
        "            assert self.steps_per_epoch is not None\n",
        "            decay_steps = self.steps_per_epoch * self.decay_epochs\n",
        "            lr = ExponentialDecay(self.initial_lr, decay_steps, \n",
        "                                  self.decay_factor, staircase=True)(step)\n",
        "        elif self.lr_decay_type == 'cosine':\n",
        "            assert self.total_steps is not None\n",
        "            lr = 0.5 * self.initial_lr * (\n",
        "              1 + tf.cos(np.pi * tf.cast(step, tf.float32) / self.total_steps))\n",
        "            \n",
        "        elif self.lr_decay_type == 'linear':\n",
        "            assert self.total_steps is not None\n",
        "            lr = (1.0 - tf.cast(step, tf.float32) / self.total_steps) * self.initial_lr\n",
        "        elif self.lr_decay_type == 'constant':\n",
        "            lr = self.initial_lr\n",
        "        else:\n",
        "            assert False, 'Unknown lr_decay_type : %s' % self.lr_decay_type\n",
        "\n",
        "        if self.minimal_lr:\n",
        "            lr = tf.math.maximum(lr, self.minimal_lr)\n",
        "\n",
        "        if self.warmup_epochs:\n",
        "            warmup_steps = int(self.warmup_epochs * self.steps_per_epoch)\n",
        "            warmup_lr = (\n",
        "              self.initial_lr * tf.cast(step, tf.float32) /\n",
        "              tf.cast(warmup_steps, tf.float32))\n",
        "            lr = tf.cond(step < warmup_steps, lambda: warmup_lr, lambda: lr)\n",
        "\n",
        "        return lr\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\n",
        "            'initial_lr': self.initial_lr,\n",
        "            'steps_per_epoch': self.steps_per_epoch,\n",
        "            'lr_decay_type': self.lr_decay_type,\n",
        "            'decay_factor': self.decay_factor,\n",
        "            'decay_epochs': self.decay_epochs,\n",
        "            'total_steps': self.total_steps,\n",
        "            'warmup_epochs': self.warmup_epochs,\n",
        "            'minimal_lr': self.minimal_lr,\n",
        "        }\n",
        "\n",
        "steps_per_epoch  = np.ceil(float(len(train_len)) / batch_size) \n",
        "validation_steps = val_step \n",
        "epochs = 3\n",
        "\n",
        "lr_sched = 'cosine'\n",
        "lr_base = 0.016\n",
        "lr_min=0\n",
        "lr_decay_epoch = 2.4\n",
        "lr_warmup_epoch = 5\n",
        "lr_decay_factor = 0.97\n",
        "\n",
        "scaled_lr = lr_base * (batch_size / 256.0)\n",
        "scaled_lr_min = lr_min * (batch_size / 256.0)\n",
        "total_steps = steps_per_epoch * epochs\n",
        "\n",
        "learning_rate = WarmupLearningRateSchedule(\n",
        "    scaled_lr,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    decay_epochs=lr_decay_epoch,\n",
        "    warmup_epochs=lr_warmup_epoch,\n",
        "    decay_factor=lr_decay_factor,\n",
        "    lr_decay_type=lr_sched,\n",
        "    total_steps=total_steps,\n",
        "    minimal_lr=scaled_lr_min)\n",
        "\n",
        "from tensorflow.keras import callbacks\n",
        "callback_list = [\n",
        "       callbacks.ModelCheckpoint(\n",
        "            filepath='model.{epoch:02d}-{val_loss:.4f}.h5', \n",
        "            save_freq='epoch', verbose=1, monitor='val_loss', \n",
        "            save_weights_only=True, save_best_only=True\n",
        "       )         \n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtZCnkM3BCN5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7190dc8-72b7-46ad-8c2d-1a76935067e5"
      },
      "source": [
        "# RandomCropedSized FIX\n",
        "# Cosine Decay Radam\n",
        " \n",
        "oof_pred = []; oof_tar = []; oof_val = []; oof_names = []; oof_folds = []; history_list = []; normal_oof_pred = []; pred_max = []\n",
        "import gc\n",
        "from sklearn.model_selection import KFold\n",
        "kf = KFold(n_splits = CFG.N_FOLDS, random_state = 0)\n",
        "FILENAMES = np.array(FILENAMES)\n",
        "seed_everything(SEED)\n",
        "for fold, (tr_index, val_index) in enumerate(kf.split(FILENAMES)):    \n",
        "    TRAINING_FILENAMES, VALIDATION_FILENAMES = FILENAMES[tr_index], FILENAMES[val_index]\n",
        "    NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\n",
        "    TRAINING_IMAGES = TRAINING_FILENAMES\n",
        " \n",
        "    train_dataset = get_dataset(TRAINING_FILENAMES, labeled=True, ordered=False, repeated=True, augment=False, validation=True)\n",
        "    val_dataset = get_dataset(VALIDATION_FILENAMES, labeled=True, ordered=True, repeated=False, augment=False, validation=True)\n",
        "    STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // CFG.BATCH_SIZE\n",
        " \n",
        "    def get_model2(NET):\n",
        " \n",
        " \n",
        "        model = CovidNet()\n",
        "        model.build(input_shape=(None, 512, 512, 3))\n",
        "\n",
        "        opt = CosineDecayRAdam(learning_rate=CFG.LEARNING_RATE, total_steps=int(STEPS_PER_EPOCH*CFG.EPOCHS), warmup_proportion=0.1, min_lr=2e-6)\n",
        "        \n",
        "        opt = tfa.optimizers.Lookahead(opt)\n",
        "        model.compile(loss = {'clss': tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
        "                              'segg': tf.keras.losses.BinaryCrossentropy(from_logits=True)},\n",
        "                      metrics = {'clss': [tf.keras.metrics.AUC(multi_label=True),]},\n",
        "                      optimizer = opt)\n",
        "        #opt =tf.keras.optimizers.Adam(learning_rate=CFG.LEARNING_RATE) \n",
        "        \n",
        "        return model\n",
        "    \n",
        "    if DEVICE=='TPU':\n",
        "        if tpu: tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    tf.keras.backend.clear_session()\n",
        "    with strategy.scope():\n",
        "        model = get_model2(CFG.NET)\n",
        "    print(f\"Efficient Model{CFG.NET} has been loaded \")\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(os.path.join(ROOT_PATH, f\"EFF{CFG.NET}_COVID19_MASK_{CFG.MASK_OBJ_WIDTH}_WIDTH_{CFG.OBJ_WIDTH}_HEIGHT_{CFG.OBJ_HEIGHT}_fold{fold}.h5\"), \n",
        "                                                    monitor = 'val_clss_auc', \n",
        "                                                    save_best_only = True,\n",
        "                                                    mode = 'max')\n",
        "    history = model.fit(train_dataset,  \n",
        "                        steps_per_epoch = STEPS_PER_EPOCH,\n",
        "                        epochs = CFG.EPOCHS,\n",
        "                        callbacks = [checkpoint],\n",
        "                        validation_data = val_dataset,\n",
        "                        verbose = 1,\n",
        "                        ).history\n",
        "    print(f\"#### FOLD {fold+1} without TTA VAL_AUC = {np.max(history['val_clss_auc']):.3f}\")\n",
        "    del model\n",
        "    gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1267, 1267, 1267, 1266]\n",
            "WARNING:tensorflow:TPU system grpc://10.12.153.26:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n",
            "WARNING:tensorflow:TPU system grpc://10.12.153.26:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.12.153.26:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.12.153.26:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Efficient Model7 has been loaded \n",
            "Epoch 1/20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSXjYZ-Rewt-"
      },
      "source": [
        "#Inference B4512"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eZByBYeuDMi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "495698f5-7fc9-4250-ce80-243998f83fb3"
      },
      "source": [
        "ds_test = get_test_dataset(TEST_FILENAMES, return_image_name=True)\n",
        "test_gogo = np.array([img_name.numpy().decode('utf-8') for img, img_name in iter(ds_test.unbatch())])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-07b08a23789b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_test_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_FILENAMES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_image_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_gogo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'TEST_FILENAMES' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hCPkcNNNrPi",
        "outputId": "dc5859a9-e2a1-4aa7-cfee-a3d36289a60f"
      },
      "source": [
        "len(test_gogo)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2986"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaX722tJhYXh",
        "outputId": "e822843b-c154-4cd5-ef92-308c2095089c"
      },
      "source": [
        "ds_test = get_test_dataset(TEST_FILENAMES, return_image_name=True)\n",
        "sub_names = np.array([img_name.numpy().decode('utf-8') for img, img_name in iter(ds_test.unbatch())])\n",
        "sub_names"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['002a34c58c5b758217ed1f584ccbcfe9',\n",
              "       '004f33259ee4aef671c2b95d54e4be68',\n",
              "       '008bdde2af2462e86fd373a445d0f4cd', ...,\n",
              "       'ffaa288c8abca300974f043b57d81521',\n",
              "       'ffc441e0c8b7153844047483a577e7c3',\n",
              "       'ffccf1709d0081d122a1d1f9edbefdf1'], dtype='<U32')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 222
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuMFk-cMhmly"
      },
      "source": [
        "df_total_sub_names = pd.DataFrame()\n",
        "df_total_sub_names['image_name'] = sub_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "6LOiSoo6hnfA",
        "outputId": "cb4d6f24-3fb7-4227-85da-d02609af2cce"
      },
      "source": [
        "df_total_sub_names"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>002a34c58c5b758217ed1f584ccbcfe9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>004f33259ee4aef671c2b95d54e4be68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>008bdde2af2462e86fd373a445d0f4cd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>009bc039326338823ca3aa84381f17f1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00a2145de1886cb9eb88869c85d74080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2995</th>\n",
              "      <td>ff91fb82429a27521bbec8569b041f02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2996</th>\n",
              "      <td>ff9fcc4087ed5e941209aa3fa948e364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2997</th>\n",
              "      <td>ffaa288c8abca300974f043b57d81521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2998</th>\n",
              "      <td>ffc441e0c8b7153844047483a577e7c3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2999</th>\n",
              "      <td>ffccf1709d0081d122a1d1f9edbefdf1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3000 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                            image_name\n",
              "0     002a34c58c5b758217ed1f584ccbcfe9\n",
              "1     004f33259ee4aef671c2b95d54e4be68\n",
              "2     008bdde2af2462e86fd373a445d0f4cd\n",
              "3     009bc039326338823ca3aa84381f17f1\n",
              "4     00a2145de1886cb9eb88869c85d74080\n",
              "...                                ...\n",
              "2995  ff91fb82429a27521bbec8569b041f02\n",
              "2996  ff9fcc4087ed5e941209aa3fa948e364\n",
              "2997  ffaa288c8abca300974f043b57d81521\n",
              "2998  ffc441e0c8b7153844047483a577e7c3\n",
              "2999  ffccf1709d0081d122a1d1f9edbefdf1\n",
              "\n",
              "[3000 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 224
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "DvkmEybnhuxB",
        "outputId": "a9aa89a9-9785-4caf-d25f-9f91c82c5ed8"
      },
      "source": [
        "df_total_pred_sub_probs = pd.DataFrame(pred_sub_prob, columns=[f\"class{x}\" for x in range(15)])\n",
        "df_sub = pd.concat([df_total_sub_names, df_total_pred_sub_probs], axis=1)\n",
        "df_sub\n",
        "#df_sub.to_csv(os.path.join(ROOT_PATH,f'VINBIG_B4512_SUB.csv'),index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_name</th>\n",
              "      <th>class0</th>\n",
              "      <th>class1</th>\n",
              "      <th>class2</th>\n",
              "      <th>class3</th>\n",
              "      <th>class4</th>\n",
              "      <th>class5</th>\n",
              "      <th>class6</th>\n",
              "      <th>class7</th>\n",
              "      <th>class8</th>\n",
              "      <th>class9</th>\n",
              "      <th>class10</th>\n",
              "      <th>class11</th>\n",
              "      <th>class12</th>\n",
              "      <th>class13</th>\n",
              "      <th>class14</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>002a34c58c5b758217ed1f584ccbcfe9</td>\n",
              "      <td>0.000875</td>\n",
              "      <td>0.000712</td>\n",
              "      <td>0.000914</td>\n",
              "      <td>0.002111</td>\n",
              "      <td>0.001013</td>\n",
              "      <td>0.000697</td>\n",
              "      <td>0.001526</td>\n",
              "      <td>0.002502</td>\n",
              "      <td>0.001551</td>\n",
              "      <td>0.001744</td>\n",
              "      <td>0.001017</td>\n",
              "      <td>0.005805</td>\n",
              "      <td>0.000549</td>\n",
              "      <td>0.001309</td>\n",
              "      <td>0.195280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>004f33259ee4aef671c2b95d54e4be68</td>\n",
              "      <td>0.001957</td>\n",
              "      <td>0.001040</td>\n",
              "      <td>0.002430</td>\n",
              "      <td>0.000818</td>\n",
              "      <td>0.000933</td>\n",
              "      <td>0.000613</td>\n",
              "      <td>0.000899</td>\n",
              "      <td>0.001299</td>\n",
              "      <td>0.002292</td>\n",
              "      <td>0.001937</td>\n",
              "      <td>0.001803</td>\n",
              "      <td>0.003263</td>\n",
              "      <td>0.000503</td>\n",
              "      <td>0.000784</td>\n",
              "      <td>0.197328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>008bdde2af2462e86fd373a445d0f4cd</td>\n",
              "      <td>0.174792</td>\n",
              "      <td>0.001424</td>\n",
              "      <td>0.007077</td>\n",
              "      <td>0.122213</td>\n",
              "      <td>0.002034</td>\n",
              "      <td>0.004011</td>\n",
              "      <td>0.006038</td>\n",
              "      <td>0.022974</td>\n",
              "      <td>0.007495</td>\n",
              "      <td>0.016485</td>\n",
              "      <td>0.003327</td>\n",
              "      <td>0.023178</td>\n",
              "      <td>0.000824</td>\n",
              "      <td>0.018443</td>\n",
              "      <td>0.033101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>009bc039326338823ca3aa84381f17f1</td>\n",
              "      <td>0.000378</td>\n",
              "      <td>0.000667</td>\n",
              "      <td>0.000435</td>\n",
              "      <td>0.000579</td>\n",
              "      <td>0.001003</td>\n",
              "      <td>0.000735</td>\n",
              "      <td>0.000990</td>\n",
              "      <td>0.001040</td>\n",
              "      <td>0.000758</td>\n",
              "      <td>0.000430</td>\n",
              "      <td>0.000802</td>\n",
              "      <td>0.000526</td>\n",
              "      <td>0.000415</td>\n",
              "      <td>0.000440</td>\n",
              "      <td>0.199561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00a2145de1886cb9eb88869c85d74080</td>\n",
              "      <td>0.086371</td>\n",
              "      <td>0.000703</td>\n",
              "      <td>0.002839</td>\n",
              "      <td>0.140198</td>\n",
              "      <td>0.002009</td>\n",
              "      <td>0.000702</td>\n",
              "      <td>0.002813</td>\n",
              "      <td>0.008863</td>\n",
              "      <td>0.003644</td>\n",
              "      <td>0.006245</td>\n",
              "      <td>0.003057</td>\n",
              "      <td>0.019245</td>\n",
              "      <td>0.001014</td>\n",
              "      <td>0.005706</td>\n",
              "      <td>0.062666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2995</th>\n",
              "      <td>ff91fb82429a27521bbec8569b041f02</td>\n",
              "      <td>0.169418</td>\n",
              "      <td>0.001050</td>\n",
              "      <td>0.034280</td>\n",
              "      <td>0.107503</td>\n",
              "      <td>0.099559</td>\n",
              "      <td>0.004379</td>\n",
              "      <td>0.036410</td>\n",
              "      <td>0.178246</td>\n",
              "      <td>0.184320</td>\n",
              "      <td>0.069109</td>\n",
              "      <td>0.007016</td>\n",
              "      <td>0.037167</td>\n",
              "      <td>0.001567</td>\n",
              "      <td>0.069370</td>\n",
              "      <td>0.002304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2996</th>\n",
              "      <td>ff9fcc4087ed5e941209aa3fa948e364</td>\n",
              "      <td>0.195568</td>\n",
              "      <td>0.001500</td>\n",
              "      <td>0.008906</td>\n",
              "      <td>0.118982</td>\n",
              "      <td>0.001435</td>\n",
              "      <td>0.002094</td>\n",
              "      <td>0.002883</td>\n",
              "      <td>0.007362</td>\n",
              "      <td>0.006666</td>\n",
              "      <td>0.011426</td>\n",
              "      <td>0.005162</td>\n",
              "      <td>0.045798</td>\n",
              "      <td>0.000817</td>\n",
              "      <td>0.026318</td>\n",
              "      <td>0.010865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2997</th>\n",
              "      <td>ffaa288c8abca300974f043b57d81521</td>\n",
              "      <td>0.001954</td>\n",
              "      <td>0.002920</td>\n",
              "      <td>0.000992</td>\n",
              "      <td>0.001406</td>\n",
              "      <td>0.004295</td>\n",
              "      <td>0.005929</td>\n",
              "      <td>0.011130</td>\n",
              "      <td>0.022830</td>\n",
              "      <td>0.001263</td>\n",
              "      <td>0.005433</td>\n",
              "      <td>0.125751</td>\n",
              "      <td>0.071830</td>\n",
              "      <td>0.002736</td>\n",
              "      <td>0.005590</td>\n",
              "      <td>0.162730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2998</th>\n",
              "      <td>ffc441e0c8b7153844047483a577e7c3</td>\n",
              "      <td>0.018895</td>\n",
              "      <td>0.000734</td>\n",
              "      <td>0.001783</td>\n",
              "      <td>0.021862</td>\n",
              "      <td>0.001242</td>\n",
              "      <td>0.000793</td>\n",
              "      <td>0.002564</td>\n",
              "      <td>0.004114</td>\n",
              "      <td>0.002283</td>\n",
              "      <td>0.002916</td>\n",
              "      <td>0.001685</td>\n",
              "      <td>0.003270</td>\n",
              "      <td>0.000978</td>\n",
              "      <td>0.003354</td>\n",
              "      <td>0.163176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2999</th>\n",
              "      <td>ffccf1709d0081d122a1d1f9edbefdf1</td>\n",
              "      <td>0.145861</td>\n",
              "      <td>0.095181</td>\n",
              "      <td>0.034802</td>\n",
              "      <td>0.004557</td>\n",
              "      <td>0.008672</td>\n",
              "      <td>0.045669</td>\n",
              "      <td>0.050831</td>\n",
              "      <td>0.070977</td>\n",
              "      <td>0.008625</td>\n",
              "      <td>0.026598</td>\n",
              "      <td>0.189738</td>\n",
              "      <td>0.199768</td>\n",
              "      <td>0.004524</td>\n",
              "      <td>0.194726</td>\n",
              "      <td>0.000403</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3000 rows × 16 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                            image_name    class0  ...   class13   class14\n",
              "0     002a34c58c5b758217ed1f584ccbcfe9  0.000875  ...  0.001309  0.195280\n",
              "1     004f33259ee4aef671c2b95d54e4be68  0.001957  ...  0.000784  0.197328\n",
              "2     008bdde2af2462e86fd373a445d0f4cd  0.174792  ...  0.018443  0.033101\n",
              "3     009bc039326338823ca3aa84381f17f1  0.000378  ...  0.000440  0.199561\n",
              "4     00a2145de1886cb9eb88869c85d74080  0.086371  ...  0.005706  0.062666\n",
              "...                                ...       ...  ...       ...       ...\n",
              "2995  ff91fb82429a27521bbec8569b041f02  0.169418  ...  0.069370  0.002304\n",
              "2996  ff9fcc4087ed5e941209aa3fa948e364  0.195568  ...  0.026318  0.010865\n",
              "2997  ffaa288c8abca300974f043b57d81521  0.001954  ...  0.005590  0.162730\n",
              "2998  ffc441e0c8b7153844047483a577e7c3  0.018895  ...  0.003354  0.163176\n",
              "2999  ffccf1709d0081d122a1d1f9edbefdf1  0.145861  ...  0.194726  0.000403\n",
              "\n",
              "[3000 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 225
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HAnbQUN5qXv"
      },
      "source": [
        "TTA = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPmlTjd8exx8"
      },
      "source": [
        "#model prediction\n",
        "from sklearn.metrics import average_precision_score\n",
        "TEST_FILENAMES = FILENAMES\n",
        "ds_test = get_test_dataset(TEST_FILENAMES, return_image_name=True)\n",
        "sub_names = np.array([img_name.numpy().decode('utf-8') for img, img_name in iter(ds_test.unbatch())])\n",
        "\n",
        "oof_pred = []; oof_tar = []; oof_val = []; oof_names = []; oof_folds = [];\n",
        "history_list = []; normal_oof_pred = []; pred_max = []; pred_probs = []; pred_sub_probs = [];\n",
        "sub_pred = [];\n",
        "pred_sub_prob = np.zeros(shape=(count_data_items(TEST_FILENAMES), CFG.NUMBER_OF_CLASSES))\n",
        "def get_model2(NET):\n",
        " \n",
        " \n",
        "        inp = tf.keras.layers.Input(shape = (CFG.OBJ_HEIGHT,CFG.OBJ_WIDTH, 3), name = 'inp1')\n",
        "        effnet = effnets[NET](weights = 'imagenet', include_top = False, pooling='avg')\n",
        "        for layer in effnet.layers:\n",
        "            if 'bn' in layer.name:\n",
        "                layer.trainable = True\n",
        "        \n",
        "        x0 = effnet(inp)\n",
        "        x0 = tf.keras.layers.Dropout(0.5)(x0)\n",
        "        x0 = tf.keras.layers.Dense(64, activation='relu')(x0)\n",
        "        x = tf.keras.layers.Dense(CFG.NUMBER_OF_CLASSES, activation='softmax', dtype='float32')(x0)\n",
        " \n",
        "        model = tf.keras.models.Model(inputs = inp, outputs = x)\n",
        "        opt = CosineDecayRAdam(learning_rate=CFG.LEARNING_RATE, total_steps=500, warmup_proportion=0.1, min_lr=2e-6)\n",
        "        \n",
        "        opt = tfa.optimizers.Lookahead(opt)\n",
        "        #opt =tf.keras.optimizers.Adam(learning_rate=CFG.LEARNING_RATE)\n",
        "        model.compile(\n",
        "            optimizer = opt,\n",
        "            loss = 'categorical_crossentropy',\n",
        "            metrics = [tf.keras.metrics.AUC(multi_label=True)]\n",
        "            ) \n",
        "        \n",
        "        return model\n",
        "\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "kf = KFold(n_splits = CFG.N_FOLDS, random_state = 0)\n",
        "FILENAMES = np.array(FILENAMES)\n",
        "\n",
        "for fold, (tr_index, val_index) in enumerate(kf.split(FILENAMES)):\n",
        "    \n",
        "    if DEVICE=='TPU':\n",
        "        if tpu: tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    with strategy.scope():\n",
        "        model = get_model2(CFG.NET)\n",
        "    \n",
        "    TRAINING_FILENAMES, VALIDATION_FILENAMES = FILENAMES[tr_index], FILENAMES[val_index]\n",
        "    NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\n",
        "    STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // CFG.BATCH_SIZE\n",
        "    #val_dataset = get_dataset(VALIDATION_FILENAMES, labeled=True, ordered=True, repeated=False, augment=False, validation=True)\n",
        "    seed_everything(SEED)\n",
        "    model.load_weights(os.path.join(ROOT_PATH, f\"COVID19{CFG.NET}_WIDTH_{CFG.OBJ_WIDTH}_HEIGHT_{CFG.OBJ_HEIGHT}_fold{fold}.h5\"))\n",
        "    print(f\"Efficient Model{CFG.NET} has been loaded \")\n",
        "    for layer in model.layers:\n",
        "        layer.trainable = False                   \n",
        "    \n",
        "    ct_valid = count_data_items(VALIDATION_FILENAMES)\n",
        "    \n",
        "    if TTA:\n",
        "########## TTA\n",
        "    ## GET NORMAL OOF\n",
        "        for i in range(CFG.TTA_NUM+1):\n",
        "            if i == 0:\n",
        "                ds_valid = get_dataset(VALIDATION_FILENAMES, labeled=True, ordered=True, repeated=False, augment=False, validation=True)\n",
        "                pred_prob = model.predict(ds_valid, verbose=1) / (CFG.TTA_NUM + 1)\n",
        "            else:\n",
        "                ds_valid = get_dataset_for_tta(VALIDATION_FILENAMES, labeled=True, ordered=True, repeated=False, augment=False, validation=True)\n",
        "                pred_prob += model.predict(ds_valid, verbose=1) / (CFG.TTA_NUM + 1)\n",
        "\n",
        "########## NO TTA\n",
        "    else:\n",
        "        ds_valid = get_dataset(VALIDATION_FILENAMES, labeled=True, ordered=True, repeated=False, augment=False, validation=True)\n",
        "        pred_prob = model.predict(ds_valid, verbose=1)\n",
        "    \n",
        "    pred_probs.append(pred_prob)\n",
        "\n",
        "\n",
        "    ds_valid = get_dataset(VALIDATION_FILENAMES, labeled=True, ordered=True, repeated=False, augment=False, validation=True)\n",
        "    oof_tar.append(np.array([target.numpy() for img, target in iter(ds_valid.unbatch())]))\n",
        "    oof_folds.append(np.ones_like(oof_tar[-1], dtype='int8')*fold)\n",
        "    ds = get_dataset(VALIDATION_FILENAMES, labeled=False, ordered=True, repeated=False, augment=False, validation=True)\n",
        "    oof_names.append(np.array([img_name.numpy().decode('utf-8') for img, img_name in iter(ds.unbatch())]))\n",
        "    \n",
        "    ds_test = get_test_dataset(TEST_FILENAMES, return_image_name=False)\n",
        "    pred_sub_prob += model.predict(ds_test, verbose=1) / CFG.N_FOLDS\n",
        "    #pred_sub_probs.append(pred_sub_prob)\n",
        "\n",
        "    #ds_test = get_test_dataset(TEST_FILENAMES, return_image_name=True)\n",
        "    #sub_names.append(np.array([img_name.numpy().decode('utf-8') for img, img_name in iter(ds_test.unbatch())]))\n",
        "\n",
        "\n",
        "\n",
        "    del model\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "true = np.concatenate(oof_tar);\n",
        "names = np.concatenate(oof_names); folds = np.concatenate(oof_folds)\n",
        "pred_probs_oof = np.concatenate(pred_probs);\n",
        "\n",
        "#total_sub_names = np.concatenate(sub_names)\n",
        "#total_pred_sub_probs = np.concatenate(pred_sub_probs)\n",
        "\n",
        "df_total_sub_names = pd.DataFrame()\n",
        "df_total_sub_names['image_name'] = sub_names\n",
        "df_total_pred_sub_probs = pd.DataFrame(pred_sub_prob, columns=[f\"class{x}\" for x in range(CFG.NUMBER_OF_CLASSES)])\n",
        "df_sub = pd.concat([df_total_sub_names, df_total_pred_sub_probs], axis=1)\n",
        "df_sub.to_csv(os.path.join(ROOT_PATH,f'(sub)COVID19{CFG.NET}_WIDTH_{CFG.OBJ_WIDTH}_HEIGHT_{CFG.OBJ_HEIGHT}_fold{fold}.csv'),index=False)\n",
        "print(df_sub)\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df_image = pd.DataFrame()\n",
        "df_image['image_name'] = names\n",
        "df_target = pd.DataFrame(true, columns=[f\"target{x}\" for x in range(CFG.NUMBER_OF_CLASSES)])\n",
        "\n",
        "df_fold = pd.DataFrame()\n",
        "df_fold['fold'] = folds[:,0]\n",
        "df_pred_probs = pd.DataFrame(pred_probs_oof, columns=[f\"class{x}\" for x in range(CFG.NUMBER_OF_CLASSES)])\n",
        "df_oof = pd.concat([df_image, df_target, df_pred_probs, df_fold], axis=1)\n",
        "df_oof.to_csv(os.path.join(ROOT_PATH,f'(oof)COVID19{CFG.NET}_WIDTH_{CFG.OBJ_WIDTH}_HEIGHT_{CFG.OBJ_HEIGHT}_fold{fold}.csv.csv'),index=False)\n",
        "\n",
        "ind_class_roc = []\n",
        "ind_class_ap = []\n",
        "for i in range(CFG.NUMBER_OF_CLASSES):\n",
        "    ind_class_roc.append(roc_auc_score(true[:,i], pred_probs_oof[:,i]))\n",
        "    ind_class_ap.append(average_precision_score(true[:,i], pred_probs_oof[:,i]))\n",
        "print(\"total auc:\",np.array(ind_class_roc).mean())\n",
        "print(\"total map:\",np.array(ind_class_ap).mean()*2/3)\n",
        "#print(f\"class {CFG.NUMBER_OF_CLASSES} auc:\", ind_class_roc[-1])\n",
        "df_oof.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "gabhdDkw7fqP",
        "outputId": "258f646b-7960-4f92-d0bc-93885be3f2e2"
      },
      "source": [
        "df_oof[['target0','target1','target2','target3']]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_name</th>\n",
              "      <th>target0</th>\n",
              "      <th>target1</th>\n",
              "      <th>target2</th>\n",
              "      <th>target3</th>\n",
              "      <th>class0</th>\n",
              "      <th>class1</th>\n",
              "      <th>class2</th>\n",
              "      <th>class3</th>\n",
              "      <th>fold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>000c3a3f293f</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.044777</td>\n",
              "      <td>0.229694</td>\n",
              "      <td>0.041547</td>\n",
              "      <td>0.683982</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>00b0891276a3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.070826</td>\n",
              "      <td>0.166704</td>\n",
              "      <td>0.025137</td>\n",
              "      <td>0.737333</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00e37a390f0f</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.007519</td>\n",
              "      <td>0.075357</td>\n",
              "      <td>0.003436</td>\n",
              "      <td>0.913689</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00e3a7e91a34</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.143055</td>\n",
              "      <td>0.262778</td>\n",
              "      <td>0.049463</td>\n",
              "      <td>0.544704</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0d4d6acc9ed3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.143055</td>\n",
              "      <td>0.262778</td>\n",
              "      <td>0.049463</td>\n",
              "      <td>0.544704</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6329</th>\n",
              "      <td>fece1740823c</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.296112</td>\n",
              "      <td>0.276682</td>\n",
              "      <td>0.046547</td>\n",
              "      <td>0.380660</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6330</th>\n",
              "      <td>ff23167d20b4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.045188</td>\n",
              "      <td>0.237923</td>\n",
              "      <td>0.060011</td>\n",
              "      <td>0.656877</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6331</th>\n",
              "      <td>ff322f8e36c4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.877586</td>\n",
              "      <td>0.057183</td>\n",
              "      <td>0.005390</td>\n",
              "      <td>0.059840</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6332</th>\n",
              "      <td>ff9f10a24c27</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.949183</td>\n",
              "      <td>0.023384</td>\n",
              "      <td>0.004172</td>\n",
              "      <td>0.023261</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6333</th>\n",
              "      <td>ffbeafe30b77</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.293803</td>\n",
              "      <td>0.321225</td>\n",
              "      <td>0.096799</td>\n",
              "      <td>0.288172</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6334 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        image_name  target0  target1  ...    class2    class3  fold\n",
              "0     000c3a3f293f      1.0      0.0  ...  0.041547  0.683982     0\n",
              "1     00b0891276a3      0.0      0.0  ...  0.025137  0.737333     0\n",
              "2     00e37a390f0f      0.0      0.0  ...  0.003436  0.913689     0\n",
              "3     00e3a7e91a34      1.0      0.0  ...  0.049463  0.544704     0\n",
              "4     0d4d6acc9ed3      1.0      0.0  ...  0.049463  0.544704     0\n",
              "...            ...      ...      ...  ...       ...       ...   ...\n",
              "6329  fece1740823c      0.0      0.0  ...  0.046547  0.380660     4\n",
              "6330  ff23167d20b4      0.0      0.0  ...  0.060011  0.656877     4\n",
              "6331  ff322f8e36c4      0.0      0.0  ...  0.005390  0.059840     4\n",
              "6332  ff9f10a24c27      1.0      0.0  ...  0.004172  0.023261     4\n",
              "6333  ffbeafe30b77      0.0      0.0  ...  0.096799  0.288172     4\n",
              "\n",
              "[6334 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "TFQiTe-u6CCx",
        "outputId": "393f346e-4ed8-4575-f56c-2eb5579d1d71"
      },
      "source": [
        "ind_class_roc = []\n",
        "ind_class_ap = []\n",
        "for i in range(CFG.NUMBER_OF_CLASSES):\n",
        "    ind_class_roc.append(roc_auc_score(true[:,i], pred_probs_oof[:,i]))\n",
        "    ind_class_ap.append(average_precision_score(true[:,i], pred_probs_oof[:,i]))\n",
        "print(\"total auc:\",np.array(ind_class_roc).mean())\n",
        "print(\"total map:\",np.array(ind_class_ap).mean()*2/3)\n",
        "#print(f\"class {CFG.NUMBER_OF_CLASSES} auc:\", ind_class_roc[-1])\n",
        "df_oof.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total auc: 0.7926285606178927\n",
            "total map: 0.3519535713592532\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_name</th>\n",
              "      <th>target0</th>\n",
              "      <th>target1</th>\n",
              "      <th>target2</th>\n",
              "      <th>target3</th>\n",
              "      <th>class0</th>\n",
              "      <th>class1</th>\n",
              "      <th>class2</th>\n",
              "      <th>class3</th>\n",
              "      <th>fold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>000c3a3f293f</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.044777</td>\n",
              "      <td>0.229694</td>\n",
              "      <td>0.041547</td>\n",
              "      <td>0.683982</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>00b0891276a3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.070826</td>\n",
              "      <td>0.166704</td>\n",
              "      <td>0.025137</td>\n",
              "      <td>0.737333</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00e37a390f0f</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.007519</td>\n",
              "      <td>0.075357</td>\n",
              "      <td>0.003436</td>\n",
              "      <td>0.913689</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00e3a7e91a34</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.143055</td>\n",
              "      <td>0.262778</td>\n",
              "      <td>0.049463</td>\n",
              "      <td>0.544704</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0d4d6acc9ed3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.143055</td>\n",
              "      <td>0.262778</td>\n",
              "      <td>0.049463</td>\n",
              "      <td>0.544704</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     image_name  target0  target1  target2  ...    class1    class2    class3  fold\n",
              "0  000c3a3f293f      1.0      0.0      0.0  ...  0.229694  0.041547  0.683982     0\n",
              "1  00b0891276a3      0.0      0.0      0.0  ...  0.166704  0.025137  0.737333     0\n",
              "2  00e37a390f0f      0.0      0.0      0.0  ...  0.075357  0.003436  0.913689     0\n",
              "3  00e3a7e91a34      1.0      0.0      0.0  ...  0.262778  0.049463  0.544704     0\n",
              "4  0d4d6acc9ed3      1.0      0.0      0.0  ...  0.262778  0.049463  0.544704     0\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    }
  ]
}